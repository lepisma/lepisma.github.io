<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2024-05-09 Thu 23:19 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Speech-First Conversational AI Revisited</title>
<meta name="author" content="Abhinav Tushar" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="/assets/css/pace.css" />
<link rel="stylesheet" type="text/css" href="/assets/css/main.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css" />
<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,800" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono" >
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="/assets/js/pace.min.js"></script>
<script src="/assets/js/jquery.zoomooz.min.js"></script>
<link rel="apple-touch-icon-precomposed" sizes="57x57" href="/assets/favicons/apple-touch-icon-57x57.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/assets/favicons/apple-touch-icon-114x114.png" />
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/assets/favicons/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicons/apple-touch-icon-144x144.png" />
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="/assets/favicons/apple-touch-icon-60x60.png" />
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="/assets/favicons/apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="/assets/favicons/apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicons/apple-touch-icon-152x152.png" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-196x196.png" sizes="196x196" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-128.png" sizes="128x128" />
<meta name="application-name" content="&nbsp;" />
<meta name="msapplication-TileColor" content="#FFFFFF" />
<meta name="msapplication-TileImage" content="/assets/favicons/mstile-144x144.png" />
<meta name="msapplication-square70x70logo" content="/assets/favicons/mstile-70x70.png" />
<meta name="msapplication-square150x150logo" content="/assets/favicons/mstile-150x150.png" />
<meta name="msapplication-wide310x150logo" content="/assets/favicons/mstile-310x150.png" />
<meta name="msapplication-square310x310logo" content="/assets/favicons/mstile-310x310.png" />
<script defer data-domain="lepisma.xyz" src="https://plausible.io/js/plausible.js"></script>
</head>
<body>
<div id="preamble" class="status">
<header>
  <div class='site-title'>
    <a href='/'>
      <img src='/assets/images/avatar32.png'>
    </a>
  </div>
  <div class='site-nav'>
    <a class='active' href='/'> blog</a>
    <a  href='/journal'> journal</a>
    <a  href='/log'> log</a>
    <a  href='/wiki'> wiki</a>
    <a href='/about'> about</a>
  </div>
  <div class='clearfix'></div>
</header>

<div class='page-header'>
  <div class='page-meta'>2023-06-30 Fri 00:00</div>
  <h1>Speech-First Conversational AI Revisited</h1>
</div>
</div>
<div id="content" class="content">
<div class='page-tags'><a href='../../../..#ml'>ml</a> <a href='../../../..#work'>work</a> <a href='../../../..#conversational-ai'>conversational-ai</a> <a href='../../../..#llm'>llm</a> <a href='../../../..#speech'>speech</a></div>
<div class="page-intro" id="org1a62a04">
<p>
This was posted first on Skit.ai's Tech blog <a href="https://tech.skit.ai/speech-first-conversational-ai-revisited//">here</a>. This page will get updates, if
needed, while the linked page will stay intact.
</p>

</div>

<p>
<span class='dropcap'>A</span>round last year, <a href="https://tech.skit.ai/speech-first-conversational-ai/">we shared our views</a> on how nuances of spoken conversations
make voicebots different than chatbots. With the recent advancements in
conversational technology, thanks to Large Language Models (LLMs), we wanted to
revisit the implications on what we call Speech-First Conversational AI. This
post is one of many such reexaminations.
</p>

<p>
We will try quoting the older blog post wherever possible, but if you haven’t
read the older post, you are encouraged to <a href="https://lepisma.xyz/2022/02/02/speech-first-conversational-ai/index.html">do so here</a> before going any further.
</p>

<div id="outline-container-sec-what’s-changed?" class="outline-2">
<h2 id="sec-what’s-changed?"><span class="section-number-2">1.</span> What’s Changed?</h2>
<div class="outline-text-2" id="text-sec-what’s-changed?">


<p>
In one line, the problem of having believable and reasonable conversations is
solved with the current generation of LLMs. You would still get factual problems
and minor niggles, but I could ask my grandmother to sit and chat with an LLM
based text bot without breaking her mental model of how human conversations
could happen, at all.
</p>

<p>
Internally, we use the phrase “text conversations are solved” to describe the
impact of LLMs on our technology. But how does this reflect in spoken
conversations? Will they also be solved? Sooner than later, sure. But there are
some details to look in that go beyond raw textual models to do this well.
</p>

<p>
Beyond the statement of “text conversations are solved”, there are more upgrades
that make us excited about their implications for spoken conversations. The most
important being the hugely improved capability to model any behavior that can be
<i>meaningfully-translated</i> in text. For example, if you want to do speech
backchannel modeling right now, you might get very far by connecting a
perception system with an LLM rather than building something else altogether.
This pattern is part of the promises of AGI, and knowing that we are getting
there gradually is very stimulating.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog" class="outline-2">
<h2 id="sec-spoken-dialog"><span class="section-number-2">2.</span> Spoken Dialog</h2>
<div class="outline-text-2" id="text-sec-spoken-dialog">


<p>
Let’s revisit the points that make spoken conversations different than textual
ones, as described in <a href="https://tech.skit.ai/speech-first-conversational-ai/">the post last year</a>. In the main, all the points are still
relevant, but the complexities involved in solutions are different now. As a
Speech AI company, this is helping us get better answers to the question of how
should we go about more natural interactions between humans and machines.
</p>
</div>

<div id="outline-container-sec-spoken-dialog/signal" class="outline-3">
<h3 id="sec-spoken-dialog/signal"><span class="section-number-3">2.1.</span> Signal</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/signal">

<blockquote>
<p>
Speech isn't merely a redundant modality, but adds valuable extra information.
Different styles of uttering the same utterance can drastically change the
meaning, something that's used a lot in human-human conversations.
</p>
</blockquote>

<p>
This is still relevant and important. While speech recognition systems have
started to become better in transcribing content, robust consumption of
non-lexical content is still a problem to solve for doing <i>spoken conversations</i>.
</p>

<p>
One of our fresh research works (releasing soon) involves utilizing <i>prosodies</i>
information along with lexical to increase language understanding and the <i>gain
we got is still significant</i>.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog/noise" class="outline-3">
<h3 id="sec-spoken-dialog/noise"><span class="section-number-3">2.2.</span> Noise</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/noise">


<p>
Speech recognition systems have come a long way from 2022. WER performance, even
in noisy audios, is extremely good and one could trust ASRs a lot more for
downstream consumption than one could do last year.
</p>

<p>
More non-speech markers, timing information, etc. are accessible easily and
accurately which could be clubbed with LLMs directly to simplify modeling
behaviors like disfluencies.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog/interaction-behavior" class="outline-3">
<h3 id="sec-spoken-dialog/interaction-behavior"><span class="section-number-3">2.3.</span> Interaction Behavior</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/interaction-behavior">


<blockquote>
<p>
We don't take turns in a half-duplex manner while talking. Even then, most
dialog management systems are designed like sequential turn-taking state
machines where party A says something, then hands over control to party B, then
takes back after B is done. The way we take turns in true spoken conversations
is more <i>full-duplex</i> and that's where a lot of interesting conversational
phenomena happen.
</p>
</blockquote>

<blockquote>
<p>
While conversing, we freely barge-in, attempt corrections, and show other
backchannel behaviors. When the other party also starts doing the same and
utilizing these both parties can have much more effective and grounded
conversations.
</p>
</blockquote>

<p>
Simulacrums of full duplex systems Google Duplex already have hinted at why this
is important. While the product impact of full-duplex conversations has been
elusive because of technology’s brittleness, with LLMs and better speech models,
the practical viability of this is pretty high now.
</p>

<p>
A natural thread of work is modeling conversations speech to speech which is
already happening in the research community. But even before perfecting that, we
can significantly get better in spoken interactions with currently available
technologies and some clever engineering.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog/runtime-performance" class="outline-3">
<h3 id="sec-spoken-dialog/runtime-performance"><span class="section-number-3">2.4.</span> Runtime Performance</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/runtime-performance">


<blockquote>
<p>
In chat conversations, model latencies and the variance over a sample don't
impact user experience a lot. Humans look at chat differently and latency, even
in seconds doesn't change the user experience as much as in voice.
</p>
</blockquote>

<blockquote>
<p>
This makes it important for the voice stack to run much faster to avoid any
violation of implicit contracts of spoken conversations.
</p>
</blockquote>

<p>
This is something where heavy LLMs don’t do well natively. Large, high-quality
models often require a GPU and optimization to meet speech latency requirements
efficiently.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog/personalization-and-adaptation" class="outline-3">
<h3 id="sec-spoken-dialog/personalization-and-adaptation"><span class="section-number-3">2.5.</span> Personalization and Adaptation</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/personalization-and-adaptation">


<blockquote>
<p>
With all the extra added richness in the signals, the potential of
personalization and adaptation goes up. A human talking to another human does
many micro-adaptations including the choice of words (common with text
conversations) and the acoustics of their voices based on the ongoing
conversation.
</p>
</blockquote>

<blockquote>
<p>
Sometimes these adaptations get ossified and form <i>sub-languages</i> that need
different approaches for designing conversations. In our experience, people
talking to voice bots talks in a different sub-language, a relatively
understudied phenomenon.
</p>
</blockquote>

<p>
As LLMs reduce the complexity and effort needed to model and design behaviors,
we should get more product-level work on this in both textual and speech
conversations. You might already see a bunch of AI talking heads, personas, etc.
with the promise of adapting to <i>you</i>. Something like this was possible earlier
but with much more effort than now.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog/response-generation" class="outline-3">
<h3 id="sec-spoken-dialog/response-generation"><span class="section-number-3">2.6.</span> Response Generation</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/response-generation">


<p>
With LLMs, the quality of responses content is extremely high and natural. And
if they are not, you can always make them so by providing a few examples.
Specifically for speech, LLMs are good substrate for modelling inputs to speech
synthesis. Instead of hand-tuning SSMLs, we can now let an LLM model high-level
markers to guide the right generation of spoken responses at the right time.
</p>

<p>
Additionally, similar to speech recognition, speech synthesis has got huge
upgrades from last year. Systems like <a href="https://github.com/suno-ai/bark">bark</a> provide a glimpse of the high quality
of utterance along with the higher order control that could be driven by an LLM.
</p>
</div>
</div>

<div id="outline-container-sec-spoken-dialog/development" class="outline-3">
<h3 id="sec-spoken-dialog/development"><span class="section-number-3">2.7.</span> Development</h3>
<div class="outline-text-3" id="text-sec-spoken-dialog/development">


<p>
This stays the same as before. Audio datasets still have more information than
text and the maintenance burden is higher.
</p>

<p>
Though there is a general reduction of complexity in the language understanding
side because of one model handling many problems together. Thus reducing
annotation, system maintenance, and other related efforts.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-what’s-next?" class="outline-2">
<h2 id="sec-what’s-next?"><span class="section-number-2">3.</span> What’s Next?</h2>
<div class="outline-text-2" id="text-sec-what’s-next?">

<p>
With higher order emergent behaviors coming in LLMs, there is a general <i>lifting
up</i> of problems that we solve in ML. All this has led to an unlocking of a sort
where everyone is rethinking the limits of automation. For a product like
ours—goal-oriented voicebots—we expect the reduction in modeling complexity to
increase the extent of automation, even for dialogs that were considered forte
of <i>human-agents</i>.
</p>

<p>
Technologically, the time is ripe to achieve great strides towards truly natural
spoken conversations with machines. Something that was always undercut because
of the, rightfully present, friction between user experience and technological
limitations. Note that the definition of <i>natural</i> here still hangs on the
evolving dynamics of human machine interactions, but we will see a phase
transition for sure.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<footer id='footer'></footer>
</div>
</body>
</html>
