
@inproceedings{rajaa_improving_2023,
	title = {Improving End-to-End {SLU} performance with Prosodic Attention and Distillation},
	url = {https://www.isca-archive.org/interspeech_2023/rajaa23_interspeech.html},
	doi = {10.21437/Interspeech.2023-1760},
	eventtitle = {Proc. Interspeech 2023},
	pages = {1114--1118},
	author = {Rajaa, Shangeth},
	urldate = {2024-08-31},
	date = {2023},
	file = {Submitted Version:/home/lepisma/Zotero/storage/3Q2ECJ5D/Rajaa - 2023 - Improving End-to-End SLU performance with Prosodic Attention and Distillation.pdf:application/pdf},
}

@inproceedings{rajaa_improving_2023-1,
	location = {Rhodes Island, Greece},
	title = {Improving Spoken Language Identification with Map-Mix},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72816-327-7},
	url = {https://ieeexplore.ieee.org/document/10095765/},
	doi = {10.1109/ICASSP49357.2023.10095765},
	eventtitle = {{ICASSP} 2023 - 2023 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {1--5},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Rajaa, Shangeth and Anandan, Kriti and Dalmia, Swaraj and Gupta, Tarun and Chng, Eng Siong},
	urldate = {2024-08-31},
	date = {2023-06-04},
	file = {Submitted Version:/home/lepisma/Zotero/storage/H9BJZSD6/Rajaa et al. - 2023 - Improving Spoken Language Identification with Map-Mix.pdf:application/pdf},
}

@inproceedings{sahu_diversity_2022,
	title = {On The Diversity of {ASR} Hypotheses In Spoken Language Understanding},
	url = {https://www.semanticscholar.org/paper/On-The-Diversity-of-ASR-Hypotheses-In-Spoken-Sahu/bddcd6532c4c4daf87f38fffbe21a82b3ef18bf5},
	abstract = {In Conversational {AI}, an Automatic Speech Recognition ({ASR}) system is used to transcribe the user’s speech, and the output of the {ASR} is passed as an input to a Spoken Language Understanding ({SLU}) system, which outputs semantic objects (such as intent, slot-act pairs, etc.). Recent work, including the state-of-the-art methods in {SLU} utilize either Word lattices or N-Best Hypotheses from the {ASR}. The intuition given for using N-Best instead of 1-Best is that the hypotheses provide extra information due to errors in the transcriptions of the {ASR} system, i.e., the performance gain is attributed to the word-error-rate ({WER}) of the {ASR}. We empirically show that the gain in using N-Best hypotheses is related to not {WER} but to the diversity of hypotheses. Code and datasets are available at this {URL}.},
	eventtitle = {{NeurIPS} 2022},
	booktitle = {I Can’t Believe It’s Not Better Workshop: Understanding Deep Learning Through Empirical Falsification},
	author = {Sahu, Surya Kant},
	urldate = {2024-08-31},
	date = {2022},
}

@inproceedings{sahu_taskmix_2022,
	location = {Online only},
	title = {{TaskMix}: Data Augmentation for Meta-Learning of Spoken Intent Understanding},
	url = {https://aclanthology.org/2022.findings-aacl.6},
	shorttitle = {{TaskMix}},
	abstract = {Meta-Learning has emerged as a research direction to better transfer knowledge from related tasks to unseen but related tasks. However, Meta-Learning requires many training tasks to learn representations that transfer well to unseen tasks; otherwise, it leads to overfitting, and the performance degenerates to worse than Multi-task Learning. We show that a state-of-the-art data augmentation method worsens this problem of overfitting when the task diversity is low. We propose a simple method, {TaskMix}, which synthesizes new tasks by linearly interpolating existing tasks. We compare {TaskMix} against many baselines on an in-house multilingual intent classification dataset of N-Best {ASR} hypotheses derived from real-life human-machine telephony utterances and two datasets derived from {MTOP}. We show that {TaskMix} outperforms baselines, alleviates overfitting when task diversity is low, and does not degrade performance even when it is high.},
	eventtitle = {Findings 2022},
	pages = {67--72},
	booktitle = {Findings of the Association for Computational Linguistics: {AACL}-{IJCNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Sahu, Surya Kant},
	editor = {He, Yulan and Ji, Heng and Li, Sujian and Liu, Yang and Chang, Chua-Hui},
	urldate = {2024-08-31},
	date = {2022-11},
	file = {Full Text PDF:/home/lepisma/Zotero/storage/M96I5SZ7/Sahu - 2022 - TaskMix Data Augmentation for Meta-Learning of Spoken Intent Understanding.pdf:application/pdf},
}

@inproceedings{ganesan_n-best_2021,
	location = {Online},
	title = {N-Best {ASR} Transformer: Enhancing {SLU} Performance using Multiple {ASR} Hypotheses},
	rights = {All rights reserved},
	url = {https://aclanthology.org/2021.acl-short.14},
	doi = {10.18653/v1/2021.acl-short.14},
	shorttitle = {N-Best {ASR} Transformer},
	abstract = {Spoken Language Understanding ({SLU}) systems parse speech into semantic structures like dialog acts and slots. This involves the use of an Automatic Speech Recognizer ({ASR}) to transcribe speech into multiple text alternatives (hypotheses). Transcription errors, ordinary in {ASRs}, impact downstream {SLU} performance negatively. Common approaches to mitigate such errors involve using richer information from the {ASR}, either in form of N-best hypotheses or word-lattices. We hypothesize that transformer models will learn better with a simpler utterance representation using the concatenation of the N-best {ASR} alternatives, where each alternative is separated by a special delimiter [{SEP}]. In our work, we test our hypothesis by using the concatenated N-best {ASR} alternatives as the input to the transformer encoder models, namely {BERT} and {XLM}-{RoBERTa}, and achieve equivalent performance to the prior state-of-the-art model on {DSTC}2 dataset. We also show that our approach significantly outperforms the prior state-of-the-art when subjected to the low data regime. Additionally, this methodology is accessible to users of third-party {ASR} {APIs} which do not provide word-lattice information.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {93--98},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Ganesan, Karthik and Bamdev, Pakhi and B, Jaivarsan and Venugopal, Amresh and Tushar, Abhinav},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2024-08-31},
	date = {2021-08},
	file = {Full Text PDF:/home/lepisma/Zotero/storage/PZ8FULR2/Ganesan et al. - 2021 - N-Best ASR Transformer Enhancing SLU Performance using Multiple ASR Hypotheses.pdf:application/pdf},
}
