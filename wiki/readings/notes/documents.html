<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2020-07-27 Mon 00:28 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Document/Paper Notes</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhinav Tushar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/assets/css/pace.css" />
<link rel="stylesheet" type="text/css" href="/assets/css/main.css" />
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css" />
<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,800" />
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono" >
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="/assets/js/pace.min.js"></script>
<script src="/assets/js/jquery.zoomooz.min.js"></script>
<link rel="apple-touch-icon-precomposed" sizes="57x57" href="/assets/favicons/apple-touch-icon-57x57.png" />
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/assets/favicons/apple-touch-icon-114x114.png" />
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/assets/favicons/apple-touch-icon-72x72.png" />
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/favicons/apple-touch-icon-144x144.png" />
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="/assets/favicons/apple-touch-icon-60x60.png" />
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="/assets/favicons/apple-touch-icon-120x120.png" />
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="/assets/favicons/apple-touch-icon-76x76.png" />
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/favicons/apple-touch-icon-152x152.png" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-196x196.png" sizes="196x196" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-32x32.png" sizes="32x32" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-16x16.png" sizes="16x16" />
<link rel="icon" type="image/png" href="/assets/favicons/favicon-128.png" sizes="128x128" />
<meta name="application-name" content="&nbsp;" />
<meta name="msapplication-TileColor" content="#FFFFFF" />
<meta name="msapplication-TileImage" content="/assets/favicons/mstile-144x144.png" />
<meta name="msapplication-square70x70logo" content="/assets/favicons/mstile-70x70.png" />
<meta name="msapplication-square150x150logo" content="/assets/favicons/mstile-150x150.png" />
<meta name="msapplication-wide310x150logo" content="/assets/favicons/mstile-310x150.png" />
<meta name="msapplication-square310x310logo" content="/assets/favicons/mstile-310x310.png" />
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="preamble" class="status">
<header>
  <div class='site-title'>
    <a href='/'>
      <img src='/assets/images/avatar32.png'>
    </a>
  </div>
  <div class='site-nav'>
    <a  href='/'> blog</a>
    <a  href='/journal'> journal</a>
    <a  href='/log'> log</a>
    <a class='active' href='/wiki'> wiki</a>
    <a href='/about'> about</a>
  </div>
  <div class='clearfix'></div>
</header>

<div class='page-header'>
  <div class='page-meta'>Last modified:  2020-07-27 Mon 00:28</div>
  <h1>Document/Paper Notes</h1>
</div>
</div>
<div id="content">
<div id='breadcrumbs'><a href='./../../index.html'>≡ index</a> / <a href='../../readings/index.html'>readings</a> / <a href='../notes/index.html'>notes</a> / documents</div>
<div id='crosslinks'></div>
<p>
This contains notes for paper-ish documents that I read.
</p>

<nav id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-evolution/complexity">1. Evolution/Complexity</a></li>
<li><a href="#sec-speech">2. Speech</a></li>
<li><a href="#sec-language">3. Language</a></li>
<li><a href="#sec-general-ai/ml">4. General AI/ML</a></li>
<li><a href="#sec-computing/programming">5. Computing/Programming</a></li>
<li><a href="#sec-misc">6. Misc</a></li>
</ul>
</div>
</nav>

<div id="outline-container-org8b88dda" class="outline-2">
<h2 id="sec-evolution/complexity"><span class="section-number-2">1</span> Evolution/Complexity</h2>
<div class="outline-text-2" id="text-sec-evolution/complexity">

</div>

<div id="outline-container-orgddb70c8" class="outline-3">
<h3 id="bearman2002networks"><span class="section-number-3">1.1</span> <span class="done READ">READ</span> Networks and history</h3>
<div class="outline-text-3" id="text-bearman2002networks">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-06-12 Wed 00:46]</span></span></p>
<pre class="example">
CUSTOM_ID: bearman2002networks
YEAR: 2002
AUTHOR: Bearman, Peter and Moody, James and Faris, Robert
</pre>

<p>
This was a really nice read. I suspect it's not that <i>nice</i> for insiders though.
Anyway, this presents a way to create <i>case</i> (in social sciences, a group of
interconnected events specifying causalities of some sort) using a few tricks
from network science.
</p>

<p>
Something a little counter intuitive for me was this idea that only robust nets
(let's coin a better term) are important since they carry on and are the only
meaningful things when you look back. I don't know enough but probably there are
two ways of looking at something like a historical event.
</p>

<ol class="org-ol">
<li><i>What actually happened</i>. I believe happenstances play roles here but shouldn't
drive derivations of general rules of thumb.</li>
<li><i>What happens</i>, taken out as a general, most probable (?), rule. The paper
focuses on this piece by presenting ways of finding non chancy causes of
events.</li>
</ol>

<p>
The problem is, I don't know which is more important to know about.
</p>
</div>
</div>

<div id="outline-container-orge88836e" class="outline-3">
<h3 id="payne2018causes"><span class="section-number-3">1.2</span> <span class="done READ">READ</span> The causes of evolvability and their evolution</h3>
<div class="outline-text-3" id="text-payne2018causes">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-01-15 Tue 00:54]</span></span></p>
<pre class="example">
CUSTOM_ID: payne2018causes
YEAR: 2018
AUTHOR: Payne, Joshua L and Wagner, Andreas
</pre>

<p>
A very recent review on the subject with a bunch of experimental results. It
talks about three "major causes of evolvability":
</p>

<ol class="org-ol">
<li>Phenotype heterogeneity</li>
<li>Robustness</li>
<li>Adaptive landscapes</li>
</ol>

<p>
And
</p>

<blockquote>
<p>
Whether they often evolve because they confer evolvability remains a
particularly challenging open question.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgae34cd9" class="outline-3">
<h3 id="pigliucci2008evolvability"><span class="section-number-3">1.3</span> <span class="done READ">READ</span> Is evolvability evolvable?</h3>
<div class="outline-text-3" id="text-pigliucci2008evolvability">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-01-06 Sun 16:01]</span></span></p>
<pre class="example">
CUSTOM_ID: pigliucci2008evolvability
YEAR: 2008
AUTHOR: Pigliucci, Massimo
</pre>

<p>
Barring for the various definitions of evolvability, we look at evolvability as
some sort of hyperparametrically derived quantity. These hyperparameters define
the phylogenic space the search will continue on <i>in the future</i>, therefore there
are the usual two arguments for evolution of evolability in the selection
setting:
</p>

<ol class="org-ol">
<li>as a side effect</li>
<li>targeted (sliding towards teleology)</li>
</ol>

<p>
Since this is a survey/opinion, there are not much technicalities here.
</p>
</div>
</div>

<div id="outline-container-org94a3f36" class="outline-3">
<h3 id="wagner2007robustness"><span class="section-number-3">1.4</span> <span class="done READ">READ</span> Robustness and evolvability: a paradox resolved</h3>
<div class="outline-text-3" id="text-wagner2007robustness">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-12-27 Thu 10:38]</span></span></p>
<pre class="example">
CUSTOM_ID: wagner2007robustness
YEAR: 2007
AUTHOR: Wagner, Andreas
</pre>

<p>
Here we put up definitions for <i>robustness</i> and <i>evolvability</i> for sequences
(genotype) and structures (phenotype). The main conclusion says that these two
values are negatively correlated for genotype, but they support each other in
case of phenotype. There are a few quantitative results on the setting of RNA
sequences and the structures they form.
</p>

<p>
The question I am interested in is, how much can this be generalized to
arbitrary levels in arbitrary systems? The key property needed to get this
working is:
</p>

<blockquote>
<p>
&#x2026;even though structure robustness increases modestly with structure frequency,
this increase is much smaller than the vast increase in the number of different
structures accessible found near a much larger neutral network.
</p>
</blockquote>

<p>
which gives
</p>

<blockquote>
<p>
&#x2026;the populations with the highly robust phenotype are more diverse, and this
increased diversity is much greater than the decreased diversity around any one
sequence.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org1389b13" class="outline-3">
<h3 id="masel2010robustness"><span class="section-number-3">1.5</span> <span class="done READ">READ</span> Robustness and evolvability</h3>
<div class="outline-text-3" id="text-masel2010robustness">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-12-15 Sat 01:28]</span></span></p>
<pre class="example">
CUSTOM_ID: masel2010robustness
YEAR: 2010
AUTHOR: Masel, Joanna and Trotter, Meredith V
</pre>

<p>
A kind of review of the ideas behind evolutionary robustness. I got a few
pointers and terminology to follow from this paper.
</p>
</div>
</div>

<div id="outline-container-org7493b96" class="outline-3">
<h3 id="hinton1987learning"><span class="section-number-3">1.6</span> <span class="done READ">READ</span> How learning can guide evolution</h3>
<div class="outline-text-3" id="text-hinton1987learning">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-11-11 Sun 17:32] </span> <span class="timestamp-kwd">DEADLINE:</span> <span class="timestamp">&lt;2018-11-11 Sun&gt;</span></span></p>
<pre class="example">
CUSTOM_ID: hinton1987learning
YEAR: 1987
AUTHOR: Hinton, Geoffrey E and Nowlan, Steven J
</pre>

<p>
Simulation of a <i>minimalistic system</i> for explaining the idea behind the searching
power of evolution + learning. Look <a href="https://egtheory.wordpress.com/2014/02/07/learning-guide-evolution/">here</a> for an argument against the specific
example taken.
</p>
</div>
</div>

<div id="outline-container-orgc3eecf7" class="outline-3">
<h3 id="kauffman1991coevolution"><span class="section-number-3">1.7</span> <span class="done READ">READ</span> Coevolution to the edge of chaos: coupled fitness landscapes, poised states, and coevolutionary avalanches</h3>
<div class="outline-text-3" id="text-kauffman1991coevolution">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-09-24 Mon 01:20]</span></span></p>
<pre class="example">
CUSTOM_ID: kauffman1991coevolution
YEAR: 1991
AUTHOR: Kauffman, Stuart A and Johnsen, Sonke
</pre>

<p>
This one uses the <i>NK model</i> to experiment with coevolution. The main idea is that
you can couple one NK landscape to another using a factor similar to K, called
C, which defines how much the other affects this guy. Sounds like a reasonable
model to represent the essence of coevolving species. An important hint that we
get is that if a metadynamics is present to <i>select</i> the value of K, then that
moves it to an attractor state where changes in the system cause avalanches
resembling the sandpile model from <a class='org-ref-reference' href="#bak1988self">bak1988self</a>.
</p>
</div>
</div>

<div id="outline-container-orgb19443c" class="outline-3">
<h3 id="langton1990computation"><span class="section-number-3">1.8</span> <span class="done READ">READ</span> Computation at the edge of chaos: phase transitions and emergent computation</h3>
<div class="outline-text-3" id="text-langton1990computation">
<pre class="example">
Custom_ID: langton1990computation
AUTHOR: Langton
JOURNAL: Physica D: Nonlinear Phenomena
YEAR: 1990
VOLUME: 42
PAGES: 12--37
</pre>

<p>
The question here focuses on how to get rules capable of computation in CAs.
Specifically, we are looking at <i>environments</i> which characterize rules that
allow:
</p>

<ol class="org-ol">
<li>Storage of information</li>
<li>Transmission</li>
<li>Interaction between the above two</li>
</ol>

<p>
Intuitively, as the rule's output entropy increases, we move from a very simple
output (more <i>storage</i>) to output with randomness (more <i>transmission</i>). In between
these two, lies the region with the right amount of signal and noise with very
large transients and this is where most of the interesting events take place.
</p>

<p>
An interesting idea involves the definition of \(\lambda\) parameter (that helps in
categorizing the rules) which is basically a discrete probability distribution
for the range of mapping function.
</p>
</div>
</div>

<div id="outline-container-org0b80397" class="outline-3">
<h3 id="bak1988self"><span class="section-number-3">1.9</span> Self-organized criticality</h3>
<div class="outline-text-3" id="text-bak1988self">
<pre class="example">
Custom_ID: bak1988self
AUTHOR: Bak, Tang \& Wiesenfeld
JOURNAL: Physical review A
YEAR: 1988
VOLUME: 38
PAGES: 364
</pre>
</div>
</div>

<div id="outline-container-orge8db7c3" class="outline-3">
<h3 id="mitchell1993revisiting"><span class="section-number-3">1.10</span> <span class="done READ">READ</span> Revisiting the edge of chaos: Evolving cellular automata to perform computations</h3>
<div class="outline-text-3" id="text-mitchell1993revisiting">
<pre class="example">
Custom_ID: mitchell1993revisiting
AUTHOR: Mitchell, Hraber \& Crutchfield
JOURNAL: arXiv preprint adap-org/9303003
YEAR: 1993
</pre>

<p>
The edge of chaos idea is pretty popular and used to explain many phenomena. A
short article criticizing that is <a href="http://bactra.org/notebooks/edge-of-chaos.html">here</a>. This is one of the papers that tried to
debunk (kind of) an experiment (<a class='org-ref-reference' href="#packard1988adaptation">packard1988adaptation</a>; this was in my
reading list for a long time) which claimed that evolving (in the GA sense) a CA
to solve computational problems gyrate it towards the edge of chaos.
</p>

<p>
It's pretty easy to see the issue since a solution to a <i>specific problem</i> (they
took majority classification) is going to have a <i>specific &lambda;</i> and that's going to
be what that is, in spite of where the critical &lambda; lies.
</p>

<p>
Other than that, this paper has some nice explanations and insights for the
results from GA. One neat trick that I haven't seen much (though I haven't seen
much) is of keeping the number of <i>elites</i> high and changing the evaluation
function on each generation. This looks like a more practical way to use GAs in
evaluation over real data set. I also like the trick where you stop at a
variable number of generations to avoid getting a rule which gets the right
answer by alternating between 0s and 1s.
</p>
</div>
</div>

<div id="outline-container-orgcb89596" class="outline-3">
<h3 id="hoffmann2018optimization"><span class="section-number-3">1.11</span> <span class="done READ">READ</span> Optimization by Self-Organized Criticality</h3>
<div class="outline-text-3" id="text-hoffmann2018optimization">
<pre class="example">
Custom_ID: hoffmann2018optimization
AUTHOR: Hoffmann \& Payton
JOURNAL: Scientific reports
YEAR: 2018
VOLUME: 8
PAGES: 2358
</pre>

<p>
I believe it is not <i>using</i> SoC in the strict sense. The key is the generation of
test patterns. Using the sandpile model, we get a reasonable
exploration/exploitation trade offs. Also, two avalanches are less likely to
occur on overlapping patches (I am going by hunches on this so can be wrong) so
it also provides a more coordinate descent-ish behavior than the regular random
patch thing. Not sure if we can say that SoC is <i>specifically</i> helping here.
</p>

<p>
There are two things. First is that this is better than the random approach
(consider <i>random patch</i> since only that is fairly comparable). This probably
needs a lot more test cases or some theoretical justification.
</p>

<p>
Second is about the optimality of the sandpile approach. How about other non <code>1/f</code>
distributions? I don't know which generating mechanisms can be employed to get
the test patterns but fishing around a bit tells me that this purity of
distribution is not that justified (consider for example the recent
<a class='org-ref-reference' href="#broido2018scale">broido2018scale</a>). The point being: if you fix an annealing schedule for
stimulated annealing based on some <i>natural</i> observation, that doesn't:
</p>
<ol class="org-ol">
<li>create a parameter-less solver, and</li>
<li>justify the <i>natural</i> observation to be the optimal</li>
</ol>

<p>
All said, I liked the thought of a random <i>object</i> (?) generator which does better
than the regular approach in the general case. If there indeed is such a
generator, this could work as an off-the-shelf technique replacing uniform
random search.
</p>
</div>
</div>

<div id="outline-container-org1df85f3" class="outline-3">
<h3 id="bertschinger2005edge"><span class="section-number-3">1.12</span> At the edge of chaos: Real-time computations and self-organized criticality in recurrent neural networks</h3>
<div class="outline-text-3" id="text-bertschinger2005edge">
<pre class="example">
Custom_ID: bertschinger2005edge
AUTHOR: Bertschinger, Natschl\"ager \& Legenstein
JOURNAL: 
YEAR: 2005
PAGES: 145--152
</pre>
</div>
</div>
</div>

<div id="outline-container-org24224da" class="outline-2">
<h2 id="sec-speech"><span class="section-number-2">2</span> Speech</h2>
<div class="outline-text-2" id="text-sec-speech">
<pre class="example">
</pre>
</div>
<div id="outline-container-org6af1093" class="outline-3">
<h3 id="zen2019libritts"><span class="section-number-3">2.1</span> <span class="done READ">READ</span> LibriTTS: A corpus derived from librispeech for text-to-speech</h3>
<div class="outline-text-3" id="text-zen2019libritts">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-06-17 Wed 00:00]</span></span></p>
<pre class="example">
CUSTOM_ID: zen2019libritts
YEAR: 2019
AUTHOR: Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J and Jia, Ye and Chen, Zhifeng and Wu, Yonghui
</pre>
</div>
</div>

<div id="outline-container-org39f1e78" class="outline-3">
<h3 id="sainath2015convolutional"><span class="section-number-3">2.2</span> <span class="done READ">READ</span> Convolutional neural networks for small-footprint keyword spotting</h3>
<div class="outline-text-3" id="text-sainath2015convolutional">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-06-13 Sat 23:59]</span></span></p>
<pre class="example">
CUSTOM_ID: sainath2015convolutional
YEAR: 2015
AUTHOR: Sainath, Tara N and Parada, Carolina
</pre>
</div>
</div>

<div id="outline-container-orgcfb1a79" class="outline-3">
<h3 id="chen2014small"><span class="section-number-3">2.3</span> <span class="done READ">READ</span> Small-footprint keyword spotting using deep neural networks</h3>
<div class="outline-text-3" id="text-chen2014small">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-06-13 Sat 23:59]</span></span></p>
<pre class="example">
CUSTOM_ID: chen2014small
YEAR: 2014
AUTHOR: Chen, Guoguo and Parada, Carolina and Heigold, Georg
</pre>
</div>
</div>

<div id="outline-container-org7ede302" class="outline-3">
<h3 id="mehrabani2018practical"><span class="section-number-3">2.4</span> <span class="done READ">READ</span> Practical Application of Domain Dependent Confidence Measurement for Spoken Language Understanding Systems</h3>
<div class="outline-text-3" id="text-mehrabani2018practical">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-06-11 Thu 23:58]</span></span></p>
<pre class="example">
CUSTOM_ID: mehrabani2018practical
YEAR: 2018
AUTHOR: Mehrabani, Mahnoosh and Thomson, David and Stern, Benjamin
</pre>

<p>
Tag noisy data and learn reject. Call center-ish use case.
</p>
</div>
</div>

<div id="outline-container-org652696a" class="outline-3">
<h3 id="carlini2018audio"><span class="section-number-3">2.5</span> <span class="done READ">READ</span> Audio adversarial examples: Targeted attacks on speech-to-text</h3>
<div class="outline-text-3" id="text-carlini2018audio">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-06-06 Sat 23:58]</span></span></p>
<pre class="example">
CUSTOM_ID: carlini2018audio
YEAR: 2018
AUTHOR: Carlini, Nicholas and Wagner, David
</pre>
</div>
</div>

<div id="outline-container-org2916109" class="outline-3">
<h3 id="shen2018natural"><span class="section-number-3">2.6</span> <span class="done READ">READ</span> Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</h3>
<div class="outline-text-3" id="text-shen2018natural">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-05-29 Fri 23:57]</span></span></p>
<pre class="example">
CUSTOM_ID: shen2018natural
YEAR: 2018
AUTHOR: Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others
</pre>
</div>
</div>

<div id="outline-container-orge03ed00" class="outline-3">
<h3 id="peters2011dialog"><span class="section-number-3">2.7</span> <span class="done READ">READ</span> Dialog Methods for Improved Alphanumeric String Capture</h3>
<div class="outline-text-3" id="text-peters2011dialog">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-03-29 Sun 13:21]</span></span></p>
<pre class="example">
CUSTOM_ID: peters2011dialog
YEAR: 2011
AUTHOR: Peters, Doug and Stubley, Peter
</pre>

<p>
Presents a way for dialog level collection of alpha numeric strings via an ASR.
Two main ideas:
</p>

<ol class="org-ol">
<li>Skip listing over n-best hypothesis across turns (attempts)</li>
<li>Chunking and confirming pieces one by one</li>
</ol>
</div>
</div>

<div id="outline-container-orgd22b873" class="outline-3">
<h3 id="han2017deep"><span class="section-number-3">2.8</span> <span class="done READ">READ</span> Deep Learning-Based Telephony Speech Recognition in the Wild.</h3>
<div class="outline-text-3" id="text-han2017deep">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-01-11 Sat 22:49]</span></span></p>
<pre class="example">
CUSTOM_ID: han2017deep
YEAR: 2017
AUTHOR: Kyu J. {Han} and Seongjun {Hahm} and Byung-Hak {Kim} and Jungsuk {Kim} and Ian R. {Lane}
</pre>

<p>
Details on CAPIO's call transcription system for 'in the Wild' data. A few nice
bits of practical information if you are working on something similar. Specially
the one about adaptation where even 10h of data gave them 5 percent point jump
(base trained on switchboard) on real data.
</p>
</div>
</div>

<div id="outline-container-org2c0443d" class="outline-3">
<h3 id="zoph2019specaugment"><span class="section-number-3">2.9</span> <span class="done READ">READ</span> SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition.</h3>
<div class="outline-text-3" id="text-zoph2019specaugment">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-01-11 Sat 22:45]</span></span></p>
<pre class="example">
CUSTOM_ID: zoph2019specaugment
YEAR: 2019
AUTHOR: Barret {Zoph} and Chung-Cheng {Chiu} and Daniel S. {Park} and Ekin Dogus {Cubuk} and Quoc V. {Le} and William {Chan} and Yu {Zhang}
</pre>

<p>
From the abstract:
</p>

<blockquote>
<p>
The augmentation policy consists of warping the features, masking blocks of
frequency channels, and masking blocks of time steps.
</p>
</blockquote>

<p>
Kaldi has this supported as a layer (applied on spectrograms) in it's nnet3
framework.
</p>
</div>
</div>

<div id="outline-container-org0c8cd48" class="outline-3">
<h3 id="dalmia2019phoneme"><span class="section-number-3">2.10</span> <span class="done READ">READ</span> Phoneme Level Language Models for Sequence Based Low Resource ASR</h3>
<div class="outline-text-3" id="text-dalmia2019phoneme">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-12-15 Sun 15:21]</span></span></p>
<pre class="example">
CUSTOM_ID: dalmia2019phoneme
YEAR: 2019
AUTHOR: Siddharth {Dalmia} and Xinjian {Li} and Alan W {Black} and Florian {Metze}
</pre>

<p>
They try using a phoneme language model (PLM) for speech recognition decoding.
There are two important pieces here. They train a single multilingual PLM
(mapping all languages to IPA) and find it doing good across languages (from
Babel dataset). Then they plug this in a CTC style model for decoding and find
that doing better than CLM (character LM) and WFST (I am assuming this is an
LG.fst) in low data setting.
</p>
</div>
</div>

<div id="outline-container-orga804ed4" class="outline-3">
<h3 id="bisani2004bootstrap"><span class="section-number-3">2.11</span> <span class="done READ">READ</span> Bootstrap estimates for confidence intervals in ASR performance evaluation</h3>
<div class="outline-text-3" id="text-bisani2004bootstrap">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-12-15 Sun 15:21]</span></span></p>
<pre class="example">
CUSTOM_ID: bisani2004bootstrap
YEAR: 2004
AUTHOR: M. {Bisani} and H. {Ney}
</pre>

<p>
The idea used in <a href="https://github.com/kaldi-asr/kaldi/blob/master/src/bin/compute-wer-bootci.cc">compute-wer-bootci</a>. Useful for comparing modifications in
speech systems when the deltas are not convincingly different.
</p>
</div>
</div>

<div id="outline-container-org380f983" class="outline-3">
<h3 id="wang2018speaker"><span class="section-number-3">2.12</span> <span class="done READ">READ</span> Speaker diarization with lstm</h3>
<div class="outline-text-3" id="text-wang2018speaker">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-11-16 Sat 14:31]</span></span></p>
<pre class="example">
CUSTOM_ID: wang2018speaker
YEAR: 2018
AUTHOR: Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopz
</pre>

<p>
d-vector + spectral clustering.
</p>
</div>
</div>

<div id="outline-container-orgd1046cb" class="outline-3">
<h3 id="xie2019utterance"><span class="section-number-3">2.13</span> <span class="done READ">READ</span> Utterance-level Aggregation for Speaker Recognition in the Wild</h3>
<div class="outline-text-3" id="text-xie2019utterance">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-11-15 Fri 19:56]</span></span></p>
<pre class="example">
CUSTOM_ID: xie2019utterance
YEAR: 2019
AUTHOR: Xie, Weidi and Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew
</pre>

<p>
Approached from more of a background reading perspective. Main idea is to use
NetVLAD, GhostVLAD style (don't know much about these at the moment) aggregation
across time instead of regular temporal average pooling.
</p>
</div>
</div>

<div id="outline-container-org418bcca" class="outline-3">
<h3 id="bredin2017pyannote"><span class="section-number-3">2.14</span> <span class="done READ">READ</span> pyannote.metrics: A Toolkit for Reproducible Evaluation, Diagnostic, and Error Analysis of Speaker Diarization Systems.</h3>
<div class="outline-text-3" id="text-bredin2017pyannote">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-11-13 Wed 14:32]</span></span></p>
<pre class="example">
CUSTOM_ID: bredin2017pyannote
YEAR: 2017
AUTHOR: Hervé {Bredin}
</pre>

<p>
Useful read for knowing metrics used in segmentation and diarization.
</p>
</div>
</div>

<div id="outline-container-org0f2b798" class="outline-3">
<h3 id="zhang2018fully"><span class="section-number-3">2.15</span> <span class="done READ">READ</span> Fully Supervised Speaker Diarization</h3>
<div class="outline-text-3" id="text-zhang2018fully">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-11-12 Tue 21:22]</span></span></p>
<pre class="example">
CUSTOM_ID: zhang2018fully
YEAR: 2018
AUTHOR: Aonan {Zhang} and Quan {Wang} and Zhenyao {Zhu} and John {Paisley} and Chong {Wang}
</pre>

<p>
A relatively recent work which has two benefits:
</p>
<ol class="org-ol">
<li>Allows unspecified number of speakers in an audio</li>
<li>Learns the clustering over speaker embeddings in a supervised way</li>
</ol>

<p>
I am not totally clear on the parameter estimation part during my first pass but
the code, which is <a href="https://github.com/google/uis-rnn">here</a>, should help.
</p>
</div>
</div>

<div id="outline-container-org6386f9d" class="outline-3">
<h3 id="sell2018diarization"><span class="section-number-3">2.16</span> <span class="done READ">READ</span> Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge.</h3>
<div class="outline-text-3" id="text-sell2018diarization">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-11-12 Tue 00:18]</span></span></p>
<pre class="example">
CUSTOM_ID: sell2018diarization
YEAR: 2018
AUTHOR: Sell, Gregory and Snyder, David and McCree, Alan and Garcia-Romero, Daniel and Villalba, Jes{\'u}s and Maciejewski, Matthew and Manohar, Vimal and Dehak, Najim and Povey, Daniel and Watanabe, Shinji and others
</pre>

<p>
I have been looking over this to get started and know about diarization a bit.
Although I got a few concepts and terminologies, this assumes you already know
your way around. There are probably better pieces if you want to clear up the
basics. <a href="https://github.com/wq2012/awesome-diarization">Here</a> is a nice resource by the way.
</p>
</div>
</div>

<div id="outline-container-org5e0368b" class="outline-3">
<h3 id="serdyuk2018towards"><span class="section-number-3">2.17</span> <span class="done READ">READ</span> Towards end-to-end spoken language understanding</h3>
<div class="outline-text-3" id="text-serdyuk2018towards">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-14 Mon 00:15]</span></span></p>
<pre class="example">
CUSTOM_ID: serdyuk2018towards
YEAR: 2018
AUTHOR: Serdyuk, Dmitriy and Wang, Yongqiang and Fuegen, Christian and Kumar, Anuj and Liu, Baiyang and Bengio, Yoshua
</pre>

<p>
Simple results (plain intent-ish classification) on going directly from audio
features to intent. An important decision, I believe, is to have bigger semantic
chunks to recur on since audio is very sample heavy.
</p>
</div>
</div>

<div id="outline-container-orgcbebb3b" class="outline-3">
<h3 id="schneider2019wav2vec"><span class="section-number-3">2.18</span> <span class="done READ">READ</span> wav2vec: Unsupervised Pre-training for Speech Recognition.</h3>
<div class="outline-text-3" id="text-schneider2019wav2vec">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-14 Mon 00:15]</span></span></p>
<pre class="example">
CUSTOM_ID: schneider2019wav2vec
YEAR: 2019
AUTHOR: Steffen {Schneider} and Alexei {Baevski} and Ronan {Collobert} and Michael {Auli}
</pre>

<p>
Not exactly what I assumed an x2vec would be with x = audio. Anyway, the idea is
to have a language model-ish system for audio frames which acts as the
featurizer for downstream tasks like, here, speech recognition. The gains are
decent. There are a few good points covered in between which drive decisions
while working with audio. Though I wonder what were the reasons for not going
with spectrums<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup> if they are replacing log-mel input in regular speech recognizer.
</p>
</div>
</div>

<div id="outline-container-orgf8c222a" class="outline-3">
<h3 id="wan2018generalized"><span class="section-number-3">2.19</span> <span class="done READ">READ</span> Generalized end-to-end loss for speaker verification</h3>
<div class="outline-text-3" id="text-wan2018generalized">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-09 Wed 01:43]</span></span></p>
<pre class="example">
CUSTOM_ID: wan2018generalized
YEAR: 2018
AUTHOR: Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez
</pre>

<p>
Leaving the speaker verification part aside, this presents a way to train
embeddings with membership constraints so that items for one identity are
grouped together and are easy to separate from the rest.
</p>
</div>
</div>

<div id="outline-container-org39562af" class="outline-3">
<h3 id="jia2018transfer"><span class="section-number-3">2.20</span> <span class="done READ">READ</span> Transfer learning from speaker verification to multispeaker text-to-speech synthesis</h3>
<div class="outline-text-3" id="text-jia2018transfer">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-09 Wed 01:20]</span></span></p>
<pre class="example">
CUSTOM_ID: jia2018transfer
YEAR: 2018
AUTHOR: Jia, Ye and Zhang, Yu and Weiss, Ron and Wang, Quan and Shen, Jonathan and Ren, Fei and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui and others
</pre>

<p>
<a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Real-Time-Voice-Cloning</a> project pointed me here. Since I don't know much about
speech synthesis at the moment, this also was a nice intro to the current
modular breakdown. Three components are involved here:
</p>

<ol class="org-ol">
<li>Discriminative speaker encoder. Trained on US English search voice data.</li>
<li>Synthesizer. Takes text to spectrogram, conditioned on speaker encoding.</li>
<li>Vocoder. Takes spectrograms to audio. Wavenet based.</li>
</ol>
</div>
</div>

<div id="outline-container-org536e092" class="outline-3">
<h3 id="likhomanenko2019needs"><span class="section-number-3">2.21</span> <span class="done READ">READ</span> Who needs words? lexicon-free speech recognition</h3>
<div class="outline-text-3" id="text-likhomanenko2019needs">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-09-15 Sun 22:33]</span></span></p>
<pre class="example">
CUSTOM_ID: likhomanenko2019needs
YEAR: 2019
AUTHOR: Likhomanenko, Tatiana and Synnaeve, Gabriel and Collobert, Ronan
</pre>

<p>
Took it from the wav2letter++ repo. Nothing <i>very</i> specific to comment on. Mostly
a results paper. I like how well ConvLM does though.
</p>
</div>
</div>

<div id="outline-container-org7070eb5" class="outline-3">
<h3 id="pratap2018wav2letter"><span class="section-number-3">2.22</span> <span class="done READ">READ</span> wav2letter++: The fastest open-source speech recognition system</h3>
<div class="outline-text-3" id="text-pratap2018wav2letter">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-09-15 Sun 22:30]</span></span></p>
<pre class="example">
CUSTOM_ID: pratap2018wav2letter
YEAR: 2018
AUTHOR: Pratap, Vineel and Hannun, Awni and Xu, Qiantong and Cai, Jeff and Kahn, Jacob and Synnaeve, Gabriel and Liptchinsky, Vitaliy and Collobert, Ronan
</pre>

<p>
Although it might not be as friendly, I like the focus on architecture and types
based guarantees in general. Kaldi just feels annoying at times.
</p>
</div>
</div>

<div id="outline-container-org9a1dde1" class="outline-3">
<h3 id="coucke2018snips"><span class="section-number-3">2.23</span> <span class="done READ">READ</span> Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</h3>
<div class="outline-text-3" id="text-coucke2018snips">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-09-15 Sun 22:26]</span></span></p>
<pre class="example">
CUSTOM_ID: coucke2018snips
YEAR: 2018
AUTHOR: Coucke, Alice and Saade, Alaa and Ball, Adrien and Bluche, Th{\'e}odore and Caulier, Alexandre and Leroy, David and Doumouro, Cl{\'e}ment and Gisselbrecht, Thibault and Caltagirone, Francesco and Lavril, Thibaut and others
</pre>

<p>
Document on how snips does their SLU in general. A nice thing that I didn't
expect was focus on the dynamic LM part. It makes sense to make updates easy and
quick on-premise as compared to keeping things mostly frozen.
</p>
</div>
</div>

<div id="outline-container-orgc2d2b26" class="outline-3">
<h3 id="rasooli2018entity"><span class="section-number-3">2.24</span> <span class="done READ">READ</span> Entity-Aware Language Model as an Unsupervised Reranker</h3>
<div class="outline-text-3" id="text-rasooli2018entity">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-05 Fri 00:05]</span></span></p>
<pre class="example">
CUSTOM_ID: rasooli2018entity
YEAR: 2018
AUTHOR: Rasooli, Mohammad Sadegh and Parthasarathy, Sarangarajan
</pre>

<p>
A few nice ideas. One was to autogenerate n-best list for a certain true text
using phonetic similarity and subsequent LM reranking. Overall idea is to
somehow introduce the relation between potential entities in the text while
ranking alternatives. The exact approach looks a little too much and I would
like to know more about how and why the decisions were taken even though
whatever they did sounds intuitive. Backstories people.
</p>
</div>
</div>

<div id="outline-container-org477df89" class="outline-3">
<h3 id="mendis2016parallelizing"><span class="section-number-3">2.25</span> <span class="done READ">READ</span> Parallelizing wfst speech decoders</h3>
<div class="outline-text-3" id="text-mendis2016parallelizing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-06-12 Wed 00:55]</span></span></p>
<pre class="example">
CUSTOM_ID: mendis2016parallelizing
YEAR: 2016
AUTHOR: Mendis, Charith and Droppo, Jasha and Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd and Zweig, Geoffrey
</pre>

<p>
I didn't get everything here mostly because of the split between AM and LM
phase. Will probably look over with more background. Overall, the idea is to
parallelize viterbi as you would think but keeping inter thread communication
very low by clumping <i>actions</i> which are mostly independent given their thread's
other actions. This clumping gains by knowledge of the graph structure which is
affected by the domain; in this case using the information about triphones.
</p>
</div>
</div>

<div id="outline-container-orgbbe82f0" class="outline-3">
<h3 id="hahn2012comparison"><span class="section-number-3">2.26</span> <span class="done READ">READ</span> Comparison of grapheme-to-phoneme methods on large pronunciation dictionaries and LVCSR tasks</h3>
<div class="outline-text-3" id="text-hahn2012comparison">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-30 Sat 20:07]</span></span></p>
<pre class="example">
CUSTOM_ID: hahn2012comparison
YEAR: 2012
AUTHOR: Hahn, Stefan and Vozila, Paul and Bisani, Maximilian
</pre>

<p>
This is mostly a comparison of statistical g2p models. I think I have a general
idea now but looks like there is a lot more to see if I start looking into
individual references. A general thread along all these models was the use of a
certain alignment (grapheme to phoneme) algorithm to get what are called
<i>graphones</i> and then train an ngrams-ish sequence model on them.
</p>
</div>
</div>

<div id="outline-container-org8d209ed" class="outline-3">
<h3 id="stolcke1996statistical"><span class="section-number-3">2.27</span> <span class="done READ">READ</span> Statistical language modeling for speech disfluencies</h3>
<div class="outline-text-3" id="text-stolcke1996statistical">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-26 Tue 00:01]</span></span></p>
<pre class="example">
CUSTOM_ID: stolcke1996statistical
YEAR: 1996
AUTHOR: Stolcke, Andreas and Shriberg, Elizabeth
</pre>

<p>
Got here from srilm's disfluency (DF) LM. The idea is to have a <i>cleanup</i> model
which models out certain common DFs, specifically filled pauses, repetitions and
deletions. Although there was not much gain, an interesting conclusion comes
with filled pauses where the DF model actually increased perplexity. The
argument being a filled pause, in most of the cases, linguistically breaks the
sentence and so the context behind it is not so useful for what follows.
</p>

<p>
Since the paper is old and also hints at a bunch of improvements in DF modeling,
I guess there might be a more recent reference around.
</p>
</div>
</div>

<div id="outline-container-org83f3aed" class="outline-3">
<h3 id="he2018streaming"><span class="section-number-3">2.28</span> <span class="done READ">READ</span> Streaming End-to-end Speech Recognition For Mobile Devices</h3>
<div class="outline-text-3" id="text-he2018streaming">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-19 Tue 00:12]</span></span></p>
<pre class="example">
CUSTOM_ID: he2018streaming
YEAR: 2018
AUTHOR: He, Yanzhang and Sainath, Tara N and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and others
</pre>

<p>
From Google's recent on-device character level Speech Recognition system. There
are a bunch of tricks used in the overall system other than the main model
itself. A few are:
</p>

<ul class="org-ul">
<li>parameter quantization (required, of course, for fast computation on mobile
device)</li>
<li>data augmentation using tts for getting numbers, proper nouns etc. right
(instead of doing fancy stuff on the model side)</li>
</ul>
</div>
</div>

<div id="outline-container-org4254d5c" class="outline-3">
<h3 id="povey2012generating"><span class="section-number-3">2.29</span> Generating exact lattices in the WFST framework</h3>
<div class="outline-text-3" id="text-povey2012generating">
<pre class="example">
CUSTOM_ID: povey2012generating
YEAR: 2012
AUTHOR: Povey, Daniel and Hannemann, Mirko and Boulianne, Gilles and Burget, Luk{\'a}{\v{s}} and Ghoshal, Arnab and Janda, Milo{\v{s}} and Karafi{\'a}t, Martin and Kombrink, Stefan and Motl{\'\i}{\v{c}}ek, Petr and Qian, Yanmin and others
</pre>
</div>
</div>

<div id="outline-container-org3b11b44" class="outline-3">
<h3 id="chen2013quantifying"><span class="section-number-3">2.30</span> <span class="done READ">READ</span> Quantifying the value of pronunciation lexicons for keyword search in lowresource languages</h3>
<div class="outline-text-3" id="text-chen2013quantifying">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-01-27 Sun 23:55]</span></span></p>
<pre class="example">
CUSTOM_ID: chen2013quantifying
YEAR: 2013
AUTHOR: Chen, Guoguo and Khudanpur, Sanjeev and Povey, Daniel and Trmal, Jan and Yarowsky, David and Yilmaz, Oguz
</pre>

<p>
In a single line, while pronunciation dictionary augmentation doesn't help that
much in WER of an LVCSR (since the OOV rates are usually low), it helps a lot in
Keyword Search.
</p>

<p>
A few other things to note are the ways to generate pronunciation and two ways
to do KWS if you already have an LVCSR system. Not surprisingly, the proxy
keyword system doesn't work that well.
</p>
</div>
</div>

<div id="outline-container-org9b7a46c" class="outline-3">
<h3 id="chiu2018state"><span class="section-number-3">2.31</span> <span class="done READ">READ</span> State-of-the-art speech recognition with sequence-to-sequence models</h3>
<div class="outline-text-3" id="text-chiu2018state">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-11-06 Tue 20:52]</span></span></p>
<pre class="example">
CUSTOM_ID: chiu2018state
YEAR: 2018
AUTHOR: Chiu, Chung-Cheng and Sainath, Tara N and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J and Rao, Kanishka and Gonina, Ekaterina and others
</pre>

<p>
Bunch of improvements on top of the LAS architecture. It feels funny that even
in end-to-end systems, we still look for <i>modular presence</i> of components like
Language Models. Maybe that helps in adding and justifying heuristics.
</p>
</div>
</div>

<div id="outline-container-org7003f89" class="outline-3">
<h3 id="mohri2008speech"><span class="section-number-3">2.32</span> Speech recognition with weighted finite-state transducers</h3>
<div class="outline-text-3" id="text-mohri2008speech">
<pre class="example">
CUSTOM_ID: mohri2008speech
YEAR: 2008
AUTHOR: Mohri, Mehryar and Pereira, Fernando and Riley, Michael
</pre>

<p>
Partial notes:
</p>
<ol class="org-ol">
<li>Composition: Transitive-ness.</li>
<li>Determinization: Removing multiple transitions on same input.</li>
<li>Minimization: Compressing to the minimal, <i>equivalent</i> automaton. Done by first
weight pushing and then running the classical algorithm.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org7eba593" class="outline-2">
<h2 id="sec-language"><span class="section-number-2">3</span> Language</h2>
<div class="outline-text-2" id="text-sec-language">
<pre class="example">
</pre>
</div>

<div id="outline-container-orge2a648e" class="outline-3">
<h3 id="mikel2019cross"><span class="section-number-3">3.1</span> <span class="done READ">READ</span> On the Cross-lingual Transferability of Monolingual Representations</h3>
<div class="outline-text-3" id="text-mikel2019cross">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-29 Tue 11:56]</span></span></p>
<pre class="example">
CUSTOM_ID: mikel2019cross
YEAR: 2019
AUTHOR: Mikel Artetxe and Sebastian Ruder and Dani Yogatama
</pre>

<ol class="org-ol">
<li>The idea of learning embeddings to <i>fit in</i> with a set of layers trained for
another language can mostly be used in other kinds of models too.</li>
<li>There was an interesting degradation in Hindi (+ Turkey) with positional
embedding on XQuAD (recoverable with added adapters). I am wondering whether
this is because</li>
</ol>

<blockquote>
<p>
transferring syntactic abstractions is more challenging than semantic abstractions. 
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org9539f4b" class="outline-3">
<h3 id="le2014distributed"><span class="section-number-3">3.2</span> <span class="done READ">READ</span> Distributed representations of sentences and documents</h3>
<div class="outline-text-3" id="text-le2014distributed">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-09 Wed 01:48]</span></span></p>
<pre class="example">
CUSTOM_ID: le2014distributed
YEAR: 2014
AUTHOR: Le, Quoc and Mikolov, Tomas
</pre>

<p>
Document vectorization paper following the general series of word2vec ones.
</p>
</div>
</div>

<div id="outline-container-orgcbd3f11" class="outline-3">
<h3 id="moody2016mixing"><span class="section-number-3">3.3</span> <span class="done READ">READ</span> Mixing dirichlet topic models and word embeddings to make lda2vec</h3>
<div class="outline-text-3" id="text-moody2016mixing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-09-25 Wed 00:30]</span></span></p>
<pre class="example">
CUSTOM_ID: moody2016mixing
YEAR: 2016
AUTHOR: Moody, Christopher E
</pre>

<p>
While I like the results, I am wondering which pieces were useful, which were
not and how do things compare to other techniques.
</p>
</div>
</div>

<div id="outline-container-org5b3da1a" class="outline-3">
<h3 id="niven2019probing"><span class="section-number-3">3.4</span> <span class="done READ">READ</span> Probing Neural Network Comprehension of Natural Language Arguments</h3>
<div class="outline-text-3" id="text-niven2019probing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-22 Mon 23:53]</span></span></p>
<pre class="example">
CUSTOM_ID: niven2019probing
YEAR: 2019
AUTHOR: Niven, Timothy and Kao, Hung-Yu
</pre>

<p>
Was up on r/ml. The abstract is short and clear enough. The idea is that ARCT
tasks have simple statistical cues which contribute in a major way for whatever
SOTA we are getting. One you balance them out, even strong models like BERT take
big hits and go to essentially random-ish performance.
</p>
</div>
</div>

<div id="outline-container-orge761be9" class="outline-3">
<h3 id="garrod2007alignment"><span class="section-number-3">3.5</span> <span class="done READ">READ</span> Alignment in dialogue</h3>
<div class="outline-text-3" id="text-garrod2007alignment">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-22 Mon 23:51]</span></span></p>
<pre class="example">
CUSTOM_ID: garrod2007alignment
YEAR: 2007
AUTHOR: Garrod, Simon and Pickering, Martin J
</pre>

<p>
Picked up this because I wanted to get general background of <i>alignment</i> as a
linguistic term. A few points I pulled out:
</p>

<ul class="org-ul">
<li>Common ground (stuff believed to be shared) is stricter than alignment which
only refers to the information that happens to be shared.</li>
<li>Ways of alignment:
<ol class="org-ol">
<li>via beliefs about one's interlocutor</li>
<li>via imitation</li>
<li>via agreements between interlocutors</li>
<li>via feedback</li>
<li>via physical co-presence</li>
</ol></li>
</ul>
</div>
</div>

<div id="outline-container-orgaf02a53" class="outline-3">
<h3 id="schatzmann2007statistical"><span class="section-number-3">3.6</span> <span class="done READ">READ</span> Statistical user simulation with a hidden agenda</h3>
<div class="outline-text-3" id="text-schatzmann2007statistical">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-22 Mon 23:46]</span></span></p>
<pre class="example">
CUSTOM_ID: schatzmann2007statistical
YEAR: 2007
AUTHOR: Schatzmann, Jost and Thomson, Blaise and Young, Steve
</pre>

<p>
This was pointed to by <a class='org-ref-reference' href="#papangelis2019collaborative">papangelis2019collaborative</a> as a way of modeling
users. Two good ideas here:
</p>

<ol class="org-ol">
<li>Stack based agenda and the general decomposition of the process.</li>
<li>Tractability piece where we try to put assumptions on various factors like
transition probabilities etc.</li>
</ol>
</div>
</div>

<div id="outline-container-org0db6f9b" class="outline-3">
<h3 id="papangelis2019collaborative"><span class="section-number-3">3.7</span> <span class="done READ">READ</span> Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning</h3>
<div class="outline-text-3" id="text-papangelis2019collaborative">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-22 Mon 23:42]</span></span></p>
<pre class="example">
CUSTOM_ID: papangelis2019collaborative
YEAR: 2019
AUTHOR: Papangelis, Alexandros and Wang, Yi-Chia and Molino, Piero and Tur, Gokhan
</pre>

<p>
This is the paper which came out with Uber's <a href="https://github.com/uber-research/plato-research-dialogue-system">plato</a>'s release. Here is what you
need to know really:
</p>

<blockquote>
<p>
Using DSTC2 as seed data, we trained NLU and NLG networks for each agent and let
the agents interact and learn online optimal dialogue policies depending on
their role (seeker or provider).
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgcd29efc" class="outline-3">
<h3 id="shah2018building"><span class="section-number-3">3.8</span> <span class="done READ">READ</span> Building a conversational agent overnight with dialogue self-play</h3>
<div class="outline-text-3" id="text-shah2018building">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-07 Sun 19:48]</span></span></p>
<pre class="example">
CUSTOM_ID: shah2018building
YEAR: 2018
AUTHOR: Shah, Pararth and Hakkani-T{\"u}r, Dilek and T{\"u}r, Gokhan and Rastogi, Abhinav and Bapna, Ankur and Nayak, Neha and Heck, Larry
</pre>

<p>
Nice ideas in here plus insights for <i>practical</i> systems like the following,
</p>

<blockquote>
<p>
Covering complex interactions is important when developing datasets to benchmark
research aimed towards building human-level dialogue systems. However, we argue
that for consumer-facing chatbots, the primary aim is reliable coverage of
critical user interactions.
</p>
</blockquote>

<p>
The generated dataset is <a href="https://github.com/google-research-datasets/simulated-dialogue">here</a> by the way.
</p>
</div>
</div>

<div id="outline-container-org3432da1" class="outline-3">
<h3 id="mallinar2018bootstrapping"><span class="section-number-3">3.9</span> <span class="done READ">READ</span> Bootstrapping Conversational Agents With Weak Supervision</h3>
<div class="outline-text-3" id="text-mallinar2018bootstrapping">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-06 Mon 23:50]</span></span></p>
<pre class="example">
CUSTOM_ID: mallinar2018bootstrapping
YEAR: 2018
AUTHOR: Mallinar, Neil and Shah, Abhishek and Ugrani, Rajendra and Gupta, Ayush and Gurusankar, Manikandan and Ho, Tin Kam and Liao, Q Vera and Zhang, Yunfeng and Bellamy, Rachel KE and Yates, Robert and others
</pre>

<p>
I like there method of mass tagging. Other than that, this is a practical
implementation of a snorkel like system.
</p>
</div>
</div>

<div id="outline-container-org47872f4" class="outline-3">
<h3 id="vlasov2018few"><span class="section-number-3">3.10</span> <span class="done READ">READ</span> Few-Shot Generalization Across Dialogue Tasks</h3>
<div class="outline-text-3" id="text-vlasov2018few">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-21 Sun 01:15]</span></span></p>
<pre class="example">
CUSTOM_ID: vlasov2018few
YEAR: 2018
AUTHOR: Vlasov, Vladimir and Drissner-Schmid, Akela and Nichol, Alan
</pre>

<p>
The idea is to put all the involved pieces in a dialog, i.e. slots, intents and
actions, in a space and then match with possible actions to do something. The
<i>key</i> idea is to have the items break into compositional pieces before embedding
so that a new domain can share along a lot of items and get along well.
</p>
</div>
</div>

<div id="outline-container-org549da7d" class="outline-3">
<h3 id="sennrich2015neural"><span class="section-number-3">3.11</span> <span class="done READ">READ</span> Neural machine translation of rare words with subword units</h3>
<div class="outline-text-3" id="text-sennrich2015neural">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-17 Wed 02:17]</span></span></p>
<pre class="example">
CUSTOM_ID: sennrich2015neural
YEAR: 2015
AUTHOR: Sennrich, Rico and Haddow, Barry and Birch, Alexandra
</pre>

<p>
This is the application of subword (BPE based) on NMT. The results mostly show
robustness and better learned handling of OOV stuff.
</p>
</div>
</div>

<div id="outline-container-org633d85f" class="outline-3">
<h3 id="kudo2018sentencepiece"><span class="section-number-3">3.12</span> <span class="done READ">READ</span> SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</h3>
<div class="outline-text-3" id="text-kudo2018sentencepiece">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-10 Wed 00:53]</span></span></p>
<pre class="example">
CUSTOM_ID: kudo2018sentencepiece
YEAR: 2018
AUTHOR: Kudo, Taku and Richardson, John
</pre>

<p>
A more rewarding read here can be the code in the <a href="https://github.com/google/sentencepiece">repository</a> itself since this
is just a short documentation on the methods implemented.
</p>
</div>
</div>

<div id="outline-container-org2771fa0" class="outline-3">
<h3 id="heinzerling2017bpemb"><span class="section-number-3">3.13</span> <span class="done READ">READ</span> Bpemb: Tokenization-free pre-trained subword embeddings in 275 languages</h3>
<div class="outline-text-3" id="text-heinzerling2017bpemb">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-10 Wed 00:48]</span></span></p>
<pre class="example">
CUSTOM_ID: heinzerling2017bpemb
YEAR: 2017
AUTHOR: Heinzerling, Benjamin and Strube, Michael
</pre>

<p>
This is mostly a report on byte-pair embedding results and comparison with other
models. Tracing back to (<a class='org-ref-reference' href="#sennrich2015neural">sennrich2015neural</a>) and the original compression
paper (<a class='org-ref-reference' href="#gage1994new">gage1994new</a>) should cover the background.
</p>
</div>
</div>

<div id="outline-container-orgd3d0e03" class="outline-3">
<h3 id="stolcke2002srilm"><span class="section-number-3">3.14</span> <span class="done READ">READ</span> SRILM-an extensible language modeling toolkit</h3>
<div class="outline-text-3" id="text-stolcke2002srilm">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-26 Tue 00:01]</span></span></p>
<pre class="example">
CUSTOM_ID: stolcke2002srilm
YEAR: 2002
AUTHOR: Stolcke, Andreas
</pre>

<p>
This is an early document on SRILM's design and development. If you are looking
for something more in-depth, just download the current tarball.
</p>
</div>
</div>

<div id="outline-container-org3bcd4be" class="outline-3">
<h3 id="goodman2001bit"><span class="section-number-3">3.15</span> <span class="done READ">READ</span> A bit of progress in language modeling</h3>
<div class="outline-text-3" id="text-goodman2001bit">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-20 Wed 19:02]</span></span></p>
<pre class="example">
CUSTOM_ID: goodman2001bit
YEAR: 2001
AUTHOR: Goodman, Joshua T
</pre>

<p>
This has a lot of nice ideas and intuitions behind tricks employed in
statistical language models. I will just write out the general topics since it's
a long paper (~73 pages for the extended version):
</p>

<ul class="org-ul">
<li>Skipping</li>
<li>Clustering</li>
<li>Caching</li>
<li>Sentence Mixture Models</li>
</ul>

<p>
At a higher level we get to know about:
</p>
<ul class="org-ul">
<li>ways of combining</li>
<li>approaching analysis</li>
<li>practical issues</li>
</ul>
</div>
</div>

<div id="outline-container-org000de1f" class="outline-3">
<h3 id="akbacak2014rapidly"><span class="section-number-3">3.16</span> <span class="done READ">READ</span> Rapidly building domain-specific entity-centric language models using semantic web knowledge sources</h3>
<div class="outline-text-3" id="text-akbacak2014rapidly">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-19 Tue 00:12]</span></span></p>
<pre class="example">
CUSTOM_ID: akbacak2014rapidly
YEAR: 2014
AUTHOR: Akbacak, Murat and Hakkani-T{\"u}r, Dilek and Tur, Gokhan
</pre>

<p>
This is focused on filtering search queries for creating language model. The
filtering that works out for them is to (after identifying a domain) go from
queries to clicked links then back to queries that went to those links. There
are a few other pieces involved but the general shape of narrowing is the same.
</p>
</div>
</div>

<div id="outline-container-orgad64607" class="outline-3">
<h3 id="andrew2019gmail"><span class="section-number-3">3.17</span> <span class="done READ">READ</span> Gmail Smart Compose: Real-Time Assisted Writing</h3>
<div class="outline-text-3" id="text-andrew2019gmail">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-04-29 Wed 23:39]</span></span></p>
<pre class="example">
CUSTOM_ID: andrew2019gmail
YEAR: 2019
AUTHOR: Andrew Dai and Benjamin Lee and Gagan Bansal and Jackie Tsay and Justin Lu and Mia Chen and Shuyuan Zhang and Tim Sohn and Yinan Wang and Yonghui Wu and Yuan Cao and Zhifeng Chen
</pre>
</div>
</div>

<div id="outline-container-org4a32511" class="outline-3">
<h3 id="wu2019self"><span class="section-number-3">3.18</span> <span class="done READ">READ</span> Self-supervised dialogue learning</h3>
<div class="outline-text-3" id="text-wu2019self">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-03-01 Sun 13:09]</span></span></p>
<pre class="example">
CUSTOM_ID: wu2019self
YEAR: 2019
AUTHOR: Wu, Jiawei and Wang, Xin and Wang, William Yang
</pre>

<p>
The self-supervision signal here is coming from a model which tries to predict
whether a provided tuple of turns is in order or not. Connecting this as the
discriminator in generative-discriminative dialog systems they find better
results.
</p>
</div>
</div>

<div id="outline-container-org1a9973c" class="outline-3">
<h3 id="hancock2019learning"><span class="section-number-3">3.19</span> <span class="done READ">READ</span> Learning from Dialogue after Deployment: Feed Yourself, Chatbot!</h3>
<div class="outline-text-3" id="text-hancock2019learning">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-02-09 Sun 21:07]</span></span></p>
<pre class="example">
CUSTOM_ID: hancock2019learning
YEAR: 2019
AUTHOR: Hancock, Braden and Bordes, Antoine and Mazare, Pierre-Emmanuel and Weston, Jason
</pre>

<p>
This is an approach to collect supervision signal from deployment data. There
are three tasks for the system (which is a chat bot doing ranking on candidate
responses):
</p>

<ol class="org-ol">
<li>Dialogue. The main task. Given the turns till now, the bot ranks which
response to utter.</li>
<li>Satisfaction. Given turns till now, last being user utterance, predict
whether the user is satisfied.</li>
<li>Feedback. After asking for feedback from the user, predict user's response
(feedback) based on the turns till now.</li>
</ol>

<p>
The models have shared weights, mostly among task 1 and 3.
</p>
</div>
</div>

<div id="outline-container-org01933da" class="outline-3">
<h3 id="vepstas2014learning"><span class="section-number-3">3.20</span> <span class="done READ">READ</span> Learning language from a large (unannotated) corpus</h3>
<div class="outline-text-3" id="text-vepstas2014learning">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-01-19 Sun 13:16]</span></span></p>
<pre class="example">
CUSTOM_ID: vepstas2014learning
YEAR: 2014
AUTHOR: Vepstas, Linas and Goertzel, Ben
</pre>

<p>
Introductory paper on the general approach used in <a href="https://github.com/opencog/learn">learn</a>. The idea is to learn
various generalizable syntactic and semantic relations from unannotated corpus.
The relations are expressed using graphs sitting on top of link grammar and
meaning text theory (MTT). While the general approach is sketched out decently
enough, there are details to filled in various steps and experiments to run (as
of the writing in 2014).
</p>

<p>
On another note, the document is a nice read because of the many interesting
ways of looking at various ideas in understanding languages and going from
syntax to reasoning via semantics.
</p>
</div>
</div>

<div id="outline-container-org2dcab51" class="outline-3">
<h3 id="sleator1995parsing"><span class="section-number-3">3.21</span> <span class="done READ">READ</span> Parsing English with a link grammar</h3>
<div class="outline-text-3" id="text-sleator1995parsing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-01-11 Sat 22:49]</span></span></p>
<pre class="example">
CUSTOM_ID: sleator1995parsing
YEAR: 1995
AUTHOR: Sleator, Daniel DK and Temperley, Davy
</pre>

<p>
Came to here via opencog's <a href="https://github.com/opencog/learn">learn</a> project. I have a patchy information about
formal grammars so this was also a nice perspective setup. Overall a link
grammar defines connectors on left and right side of a word with disjunctions
and conjunctions incorporated which then <i>link</i> together to form a sentence, under
certain constraints.
</p>

<p>
This specific paper shows the formulation and creates a parser for English,
covering many (not all) linguistics phenomena.
</p>
</div>
</div>

<div id="outline-container-org54ccae7" class="outline-3">
<h3 id="zettlemoyer2012learning"><span class="section-number-3">3.22</span> <span class="done READ">READ</span> Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</h3>
<div class="outline-text-3" id="text-zettlemoyer2012learning">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-20 Sat 20:58] </span> <span class="timestamp-kwd">DEADLINE:</span> <span class="timestamp">&lt;2018-10-16 Tue&gt;</span></span></p>
<pre class="example">
CUSTOM_ID: zettlemoyer2012learning
YEAR: 2012
AUTHOR: Zettlemoyer, Luke S and Collins, Michael
</pre>

<p>
<i>Assuming the title clarifies the goal</i>, there are three basic components here:
</p>

<ol class="org-ol">
<li>A <span class="underline">parser</span> which takes a sentence \(S\), a set of categories \(\Lambda\) and weights over
features of the derivation (generated from parsing) \(\theta\). This then generates
logical forms (\(L\)) with certain probabilities.</li>
<li>Category <span class="underline">generator</span> which takes \(S\) and its expected logical form \(L\) to
generate the categories needed to parse it to that form.</li>
<li>An <span class="underline">estimator</span> which, given the training set and a set of categories, updates
\(\theta\) to increase the score of the form getting parsed.</li>
</ol>

<p>
The interesting pieces are the representation of the logical form \(L\) (using &lambda;
calculus) and category generation and pruning. Although the generated categories
can be arbitrary, allowing for wrong grammars and such, I believe, it can be
made to work better in noisy settings if we generalize <i>parsing</i> and (maybe) the
meaning of the structurally rigid categories like \(S/NP\) using a few tricks.
</p>
</div>
</div>

<div id="outline-container-orgc839505" class="outline-3">
<h3 id="steedman1996very"><span class="section-number-3">3.23</span> <span class="done READ">READ</span> A very short introduction to CCG</h3>
<div class="outline-text-3" id="text-steedman1996very">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-16 Tue 02:21]</span></span></p>
<pre class="example">
CUSTOM_ID: steedman1996very
YEAR: 1996
AUTHOR: Steedman, Mark
</pre>

<p>
A lambda calculus formulation of <i>verb</i> (function) acts in natural text. Not sure
if I can figure out exact advantages as compared to other approaches. This
definitely has more appeal to it because of the functional forms and the tooling
they pull in with themselves.
</p>
</div>
</div>

<div id="outline-container-org8455d6d" class="outline-3">
<h3 id="benjelloun2009swoosh"><span class="section-number-3">3.24</span> <span class="done READ">READ</span> Swoosh: a generic approach to entity resolution</h3>
<div class="outline-text-3" id="text-benjelloun2009swoosh">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-07 Sun 20:23] </span> <span class="timestamp-kwd">SCHEDULED:</span> <span class="timestamp">&lt;2018-10-06 Sat&gt;</span></span></p>
<pre class="example">
CUSTOM_ID: benjelloun2009swoosh
YEAR: 2009
AUTHOR: Benjelloun, Omar and Garcia-Molina, Hector and Menestrina, David and Su, Qi and Whang, Steven Euijong and Widom, Jennifer
</pre>

<p>
The main products are optimal algorithms to do ER which minimize the number of
calls to the black box functions that actually perform the matching and merging.
To do this, we first formalize the ER problem using:
</p>

<ol class="org-ol">
<li><i>Records</i> and <i>features</i> as the data structures</li>
<li><i>Merging</i> and <i>matching</i> functions as the operations</li>
</ol>

<p>
Then we look for certain properties of a particular setting (mostly the effect
of merge and match functions). Based on whether a few of these are satisfied
(surprisingly trivial functions might not do what you expect of them), we can
reduce the number of calls to matching.
</p>
</div>
</div>

<div id="outline-container-org0d442ea" class="outline-3">
<h3 id="liu2016not"><span class="section-number-3">3.25</span> <span class="done READ">READ</span> How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation</h3>
<div class="outline-text-3" id="text-liu2016not">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-13 Sat 17:45] </span> <span class="timestamp-kwd">SCHEDULED:</span> <span class="timestamp">&lt;2018-10-06 Sat 15:00&gt;</span></span></p>
<pre class="example">
CUSTOM_ID: liu2016not
YEAR: 2016
AUTHOR: Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle
</pre>

<p>
Other than the usuals, it has decent summaries of a few metrics used for
sentence similarity.
</p>
</div>
</div>

<div id="outline-container-org69f2a2b" class="outline-3">
<h3 id="liang2015bringing"><span class="section-number-3">3.26</span> <span class="done READ">READ</span> Bringing machine learning and compositional semantics together</h3>
<div class="outline-text-3" id="text-liang2015bringing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-02 Tue 13:26]</span></span></p>
<pre class="example">
CUSTOM_ID: liang2015bringing
YEAR: 2015
AUTHOR: Liang, Percy and Potts, Christopher
</pre>

<p>
Got pointed to this while going through <a href="https://github.com/wcmac/sippycup">sippycup</a>. This presents, in a very
pedagogical way, a simple framework for ranking semantic parses using supervised
learning. The important point is that this <i>framework</i> can be applied to a lot of
problems in nlu involving different ways of structuring the logical forms and
features.
</p>
</div>
</div>


<div id="outline-container-org3ae19e4" class="outline-3">
<h3 id="banarescu2013abstract"><span class="section-number-3">3.27</span> <span class="done READ">READ</span> Abstract meaning representation for sembanking</h3>
<div class="outline-text-3" id="text-banarescu2013abstract">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-10 Sun 22:43]</span></span></p>
<pre class="example">
CUSTOM_ID: banarescu2013abstract
YEAR: 2013
AUTHOR: Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan
</pre>

<p>
I found AMR while looking into a way of breaking from the usual intent/entity
based NLU. While they are <a href="https://nlpers.blogspot.com/2014/09/amr-not-semantics-but-close-maybe.html">not perfect</a>, the specification tells you about pieces
which should (in elaborate situations) be considered at least for practical
computational language understanding.
</p>
</div>
</div>

<div id="outline-container-orga60213d" class="outline-3">
<h3 id="weilhammer2006bootstrapping"><span class="section-number-3">3.28</span> <span class="done READ">READ</span> Bootstrapping language models for dialogue systems</h3>
<div class="outline-text-3" id="text-weilhammer2006bootstrapping">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-10 Sun 22:43]</span></span></p>
<pre class="example">
CUSTOM_ID: weilhammer2006bootstrapping
YEAR: 2006
AUTHOR: Weilhammer, Karl and Stuttle, Matthew N and Young, Steve
</pre>

<p>
This is quickly getting <i>domain specific</i> LMs. The idea is to not do a lot of
manual (and perfect) text collection but start with simple grammars and get a
seed LM using the generated text. Then for more refinements, get a large LM and
do sentence selection on <i>in-the-wild</i> data to get sentences with low value of
\(PP_{seed} / PP_{large}\). These sentences and the <i>rejected</i> ones then give two more LMs
which can then be interpolated based on a validation set.
</p>

<p>
Exact steps aside, the idea (other than SLMs on grammar generated data) is to do
some sort of <i>sentence selection</i> to augment the seed LM.
</p>
</div>
</div>

<div id="outline-container-orgdbb3866" class="outline-3">
<h3 id="raghuvanshi2018developing"><span class="section-number-3">3.29</span> <span class="done READ">READ</span> Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing</h3>
<div class="outline-text-3" id="text-raghuvanshi2018developing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-01-15 Tue 01:10]</span></span></p>
<pre class="example">
CUSTOM_ID: raghuvanshi2018developing
YEAR: 2018
AUTHOR: Raghuvanshi, Arushi and Carroll, Lucien and Raghunathan, Karthik
</pre>

<p>
Doc on Mindmeld's NLU system.
</p>
</div>
</div>

<div id="outline-container-org0705ae0" class="outline-3">
<h3 id="lebret2016neural"><span class="section-number-3">3.30</span> <span class="done READ">READ</span> Neural text generation from structured data with application to the biography domain</h3>
<div class="outline-text-3" id="text-lebret2016neural">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-01-05 Sat 23:22]</span></span></p>
<pre class="example">
CUSTOM_ID: lebret2016neural
YEAR: 2016
AUTHOR: Lebret, R{\'e}mi and Grangier, David and Auli, Michael
</pre>

<p>
From wikipedia info entry (a table) for a person, they generate biographical
sentences. The way to condition on the table while doing \(P(w_i | c_{(i-1)})\) is
just indexing into (learnable) embeddings. I was looking for something more
insightful though.
</p>
</div>
</div>
</div>

<div id="outline-container-org8703779" class="outline-2">
<h2 id="sec-general-ai/ml"><span class="section-number-2">4</span> General AI/ML</h2>
<div class="outline-text-2" id="text-sec-general-ai/ml">
<pre class="example">
</pre>
</div>

<div id="outline-container-org6ac14ae" class="outline-3">
<h3 id="grathwohl2019classifier"><span class="section-number-3">4.1</span> <span class="done READ">READ</span> Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One</h3>
<div class="outline-text-3" id="text-grathwohl2019classifier">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-12-15 Sun 15:18]</span></span></p>
<pre class="example">
CUSTOM_ID: grathwohl2019classifier
YEAR: 2019
AUTHOR: Will Grathwohl and Kuan-Chieh Wang and Jörn-Henrik Jacobsen and David Duvenaud and Mohammad Norouzi and Kevin Swersky
</pre>

<p>
They take a regular classifier, pick out logits before softmax and try to
formulate an energy based model able to give \(P(x, y)\) and \(P(x)\). The
formulation itself is pretty simple with the energy function being \(E(x) =
−LogSumExp_yf_\Theta(x)[y]\). Final loss sums cross entropy (for discriminative part)
and negative log likelhood of \(P(x)\) approximated using SGLD. Check out the repo
<a href="https://github.com/wgrathwohl/JEM">here</a>.
</p>

<p>
Although the learning mechanism is a little fragile and needs work to be
generally stable, the results are neat.
</p>
</div>
</div>

<div id="outline-container-orgcd89c83" class="outline-3">
<h3 id="varma2018snuba"><span class="section-number-3">4.2</span> <span class="done READ">READ</span> Snuba: automating weak supervision to label training data</h3>
<div class="outline-text-3" id="text-varma2018snuba">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-14 Mon 00:14]</span></span></p>
<pre class="example">
CUSTOM_ID: varma2018snuba
YEAR: 2018
AUTHOR: Varma, Paroma and R{\'e}, Christopher
</pre>

<p>
This is a logical extension of <a class='org-ref-reference' href="#ratner2017snorkel">ratner2017snorkel</a>. Instead of users writing
heuristics, we go one level farther and just provide primitives (semantically
meaningful feature chunks). There are three components:
</p>

<ol class="org-ol">
<li>Synthesizer that does heuristic creation based on certain labelled dataset.</li>
<li>A pruner that picks <i>good</i> heuristics, based on certain definitions and
constraints.</li>
<li>A verifier which closes the loop by deciding when to stop, what to feed to
synthesizer etc.</li>
</ol>

<p>
While there are obvious upgrades in how we are doing everything, the general
architecture reminds me much of classical rule learning systems like <a href="https://en.wikipedia.org/wiki/Learning_classifier_system">LCS</a>.
</p>
</div>
</div>

<div id="outline-container-org62372ec" class="outline-3">
<h3 id="mignan2019one"><span class="section-number-3">4.3</span> <span class="done READ">READ</span> One neuron is more informative than a deep neural network for aftershock pattern forecasting</h3>
<div class="outline-text-3" id="text-mignan2019one">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-09 Wed 01:45]</span></span></p>
<pre class="example">
CUSTOM_ID: mignan2019one
YEAR: 2019
AUTHOR: Mignan, Arnaud and Broccardo, Marco
</pre>

<p>
Got from r/MachineLearning. Title kind of says what is there in the paper. Even
then, I would recommend skimming it just because the model is drastically
simpler than the neural-net they are comparing to.
</p>
</div>
</div>

<div id="outline-container-orgbd06b63" class="outline-3">
<h3 id="carlini2019secret"><span class="section-number-3">4.4</span> <span class="done READ">READ</span> The Secret Sharer: Evaluating and testing unintended memorization in neural networks</h3>
<div class="outline-text-3" id="text-carlini2019secret">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-09 Wed 01:31]</span></span></p>
<pre class="example">
CUSTOM_ID: carlini2019secret
YEAR: 2019
AUTHOR: Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn
</pre>

<p>
This tries to formalize the problem of <i>unintended memorization</i> in neural
networks. Notice the emphasis. There are many different ways to interpret
<i>memorization</i> and the authors here are only concerned about cases where (say)
something like a private sequence gets sucked in the memory and Mallory is able
to extract such pieces with reasonable common attacks.
</p>

<p>
Important is their metric, called <i>exposure</i>, which kind of defines how easy it is
to get a memorized piece of information out by playing around with the model
API.
</p>
</div>
</div>

<div id="outline-container-org9ff78e0" class="outline-3">
<h3 id="breck2016s"><span class="section-number-3">4.5</span> <span class="done READ">READ</span> What’s your ML Test Score? A rubric for ML production systems</h3>
<div class="outline-text-3" id="text-breck2016s">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-09 Wed 01:19]</span></span></p>
<pre class="example">
CUSTOM_ID: breck2016s
YEAR: 2016
AUTHOR: Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D
</pre>

<p>
This is a good guide to follow if you work in a production ML setting.
</p>
</div>
</div>

<div id="outline-container-org15e1a36" class="outline-3">
<h3 id="christiano2018supervising"><span class="section-number-3">4.6</span> <span class="done READ">READ</span> Supervising strong learners by amplifying weak experts</h3>
<div class="outline-text-3" id="text-christiano2018supervising">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-08-20 Tue 00:32]</span></span></p>
<pre class="example">
CUSTOM_ID: christiano2018supervising
YEAR: 2018
AUTHOR: Christiano, Paul and Shlegeris, Buck and Amodei, Dario
</pre>

<p>
This is the Iterated Amplification paper. General idea is that harder problems
can't be solved directly in a stable way so we want to use expert assistance for
breaking down things in pieces. Then an interactive process lets the to-be
trained system learn from both the ways things are broken and answer
constructed. Since this is one of the initial works, the overall framework might
change a little.
</p>

<p>
Simple algorithmic examples are provided. It will be interesting to see attempts
towards the candidate problems which are beyond human intelligence. Haven't
really followed the thread so don't know if there already is something done in
this direction.
</p>
</div>
</div>

<div id="outline-container-orgc2d6560" class="outline-3">
<h3 id="daume2009frustratingly"><span class="section-number-3">4.7</span> <span class="done READ">READ</span> Frustratingly easy domain adaptation</h3>
<div class="outline-text-3" id="text-daume2009frustratingly">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-08-20 Tue 00:31]</span></span></p>
<pre class="example">
CUSTOM_ID: daume2009frustratingly
YEAR: 2009
AUTHOR: Daum{\'e} III, Hal
</pre>

<p>
Tells you how far good insights go. The paper is really simple to follow so not
writing anything here.
</p>
</div>
</div>

<div id="outline-container-org29a0e25" class="outline-3">
<h3 id="schwartz2019green"><span class="section-number-3">4.8</span> <span class="done READ">READ</span> Green AI</h3>
<div class="outline-text-3" id="text-schwartz2019green">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-08-20 Tue 00:29]</span></span></p>
<pre class="example">
CUSTOM_ID: schwartz2019green
YEAR: 2019
AUTHOR: Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren
</pre>

<p>
A noble idea to push for.
</p>

<p>
I sometime feel bad that things which are good inherently, need to be pushed out
in a gamified way for actions to be taken.
</p>
</div>
</div>

<div id="outline-container-orge816277" class="outline-3">
<h3 id="welling2011bayesian"><span class="section-number-3">4.9</span> <span class="done READ">READ</span> Bayesian learning via stochastic gradient Langevin dynamics</h3>
<div class="outline-text-3" id="text-welling2011bayesian">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-02 Tue 22:52]</span></span></p>
<pre class="example">
CUSTOM_ID: welling2011bayesian
YEAR: 2011
AUTHOR: Welling, Max and Teh, Yee W
</pre>

<p>
The update equation here is a minibatched SGD's with a normal noise factor
\(\eta\):
</p>

<p>
\[ \Delta\theta_{t} = \frac{\epsilon_{t}}{2}  \left(\nabla \log p (\theta_{t})  + \frac{N}{n} \sum_{i=1}^{n} \nabla \log p (x_{ti} | \theta_{t}) \right) + \eta_{t} \]
</p>

<p>
Main results involve showing that, when the rate \(\epsilon\) decays following
certain properties, the noise due to minibatch dominates in the initial phase
giving us normal SGD while the \(\eta\) noise dominates in the later phase which
basically lets us sample from the posterior of \(\theta\).
</p>

<p>
The question of when to say we are in the sampling phase (so that we can start
collecting samples, taking the SGD phase as burn-in) is also answered though I
am missing some statistical tooling at the moment to appreciate it.
</p>
</div>
</div>

<div id="outline-container-org0e12d46" class="outline-3">
<h3 id="carbonell1983learning"><span class="section-number-3">4.10</span> <span class="done READ">READ</span> Learning by analogy: Formulating and generalizing plans from past experience</h3>
<div class="outline-text-3" id="text-carbonell1983learning">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-07-02 Tue 22:52]</span></span></p>
<pre class="example">
CUSTOM_ID: carbonell1983learning
YEAR: 1983
AUTHOR: Carbonell, Jaime G
</pre>

<p>
The summary overall is <i>solving problems</i> \(\equiv\) <i>learning to solve problems</i>.
This is probably in general applicable to all analogy based methods but here the
idea is also to apply <i>learnings</i> from one problem/domain to others. Two key
components are involved here:
</p>

<ol class="org-ol">
<li>Apply a <i>generic problem solver</i> (in the classical planning sense) to higher
order problems like reducing a past solution to a new solution for another
problem.</li>
<li>A learning system which helps in learning parameters for a memory table which
indexes actions based on the effects they produce.</li>
</ol>

<p>
Like in many older papers a lot of deliberations from here too are probably now
parametrized and learned.
</p>
</div>
</div>

<div id="outline-container-org8d583f3" class="outline-3">
<h3 id="liu2019large"><span class="section-number-3">4.11</span> <span class="done READ">READ</span> Large-Scale Long-Tailed Recognition in an Open World</h3>
<div class="outline-text-3" id="text-liu2019large">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-06-14 Fri 00:19]</span></span></p>
<pre class="example">
CUSTOM_ID: liu2019large
YEAR: 2019
AUTHOR: Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X
</pre>

<p>
Here we are grouping the following three tasks, their losses, metrics etc. in
one:
</p>
<ol class="org-ol">
<li>Regular (a little imbalanced) classification</li>
<li>Few shot classification</li>
<li>Out of Domain classification</li>
</ol>

<p>
Even though the system is a full network, there are planned components in there.
Two are noteworthy:
</p>
<ol class="org-ol">
<li>A distance metric, <i>reachability</i> in paper, goes to tell how different an
instance is as compared to seen examples. This helps in task 2 vs 3.</li>
<li>Memorized feature infusion which comes into picture in task 1 vs 2. Here we
put more weights for features from a memory which helps in reducing the bias
towards regular classes with large number of training samples.</li>
</ol>
</div>
</div>

<div id="outline-container-org6b648ec" class="outline-3">
<h3 id="bainczyk2017model"><span class="section-number-3">4.12</span> <span class="done READ">READ</span> Model-based testing without models: the TodoMVC case study</h3>
<div class="outline-text-3" id="text-bainczyk2017model">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-06-04 Tue 10:52]</span></span></p>
<pre class="example">
CUSTOM_ID: bainczyk2017model
YEAR: 2017
AUTHOR: Bainczyk, Alexander and Schieweck, Alexander and Steffen, Bernhard and Howar, Falk
</pre>

<p>
This is a case study of a general purpose UI testing approach. Here are the
general steps:
</p>

<ol class="org-ol">
<li>You define a set of actions that can be done.</li>
<li>Learn a mealy model (makes sense for a lot of UIs) based on exploration using
those states (I am not very sure I am using the correct phrasing for this
learning)</li>
<li>Compare with reference, among siblings etc. Probably also fuzz.</li>
</ol>

<p>
Even though there is not much in this specific paper itself, I got a general
overview of the scene and references to a few primary sources.
</p>
</div>
</div>

<div id="outline-container-org01dad0f" class="outline-3">
<h3 id="gordon2018morphnet"><span class="section-number-3">4.13</span> <span class="done READ">READ</span> Morphnet: Fast &amp; simple resource-constrained structure learning of deep networks</h3>
<div class="outline-text-3" id="text-gordon2018morphnet">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-24 Fri 23:12]</span></span></p>
<pre class="example">
CUSTOM_ID: gordon2018morphnet
YEAR: 2018
AUTHOR: Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward
</pre>

<p>
In a single line (from the appendix) what is happening is:
</p>

<blockquote>
<p>
iterative process of shrinking via a sparsifying regularizer and expanding via a
uniform multiplicative factor
</p>
</blockquote>

<p>
The regularizer is an \(L1\) over the batch norm &gamma; parameters for neurons.
</p>
</div>
</div>

<div id="outline-container-org94dcb35" class="outline-3">
<h3 id="beygelzimer2016learning"><span class="section-number-3">4.14</span> <span class="done READ">READ</span> Learning reductions that really work</h3>
<div class="outline-text-3" id="text-beygelzimer2016learning">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-22 Wed 00:01]</span></span></p>
<pre class="example">
CUSTOM_ID: beygelzimer2016learning
YEAR: 2016
AUTHOR: Beygelzimer, Alina and Daum{\'e}, Hal and Langford, John and Mineiro, Paul
</pre>

<p>
I was looking into the general ideas behind <a href="https://github.com/VowpalWabbit/vowpal_wabbit/">Vowpal Wabbit</a> and got to this
document which <i>probably</i> summarizes the whole concept of learning reductions.
</p>

<p>
An important question is how general and fundamental this whole idea really is.
Of course computational benefits are a major plus, but reductions also <i>feel</i> very
elegant.
</p>
</div>
</div>

<div id="outline-container-org2ba1f07" class="outline-3">
<h3 id="asai2019unsupervised"><span class="section-number-3">4.15</span> <span class="done READ">READ</span> Unsupervised Grounding of Plannable First-Order Logic Representation from Images</h3>
<div class="outline-text-3" id="text-asai2019unsupervised">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-12 Sun 14:10]</span></span></p>
<pre class="example">
CUSTOM_ID: asai2019unsupervised
YEAR: 2019
AUTHOR: Asai, Masataro
</pre>

<p>
This is a attempt to have interpretable representation from a neural network
that can be used with planing systems. The abstract should tell you what
problems are getting solved. The keys ideas are the following:
</p>

<ol class="org-ol">
<li>First Order State Auto Encoder (FOSAE) where the latent space represents FOL
predicates based on certain input objects and specified hyperparameters.</li>
<li>Extensive use of Gumbel-Softmax to impose unitary credit assignment.</li>
</ol>

<p>
I am not very sure how different this is from similar recent works since I
haven't followed them. But the main difference looks like focusing on discrete
representations and planning capabilities of PDDL-ish tools. Interpretability
comes as a side effect.
</p>

<p>
Since the predicates here are anonymous as of now, an interesting piece of
future work involves a bit of supervision to put <i>names</i> on things.
</p>
</div>
</div>

<div id="outline-container-org18c68ce" class="outline-3">
<h3 id="hundman2018detecting"><span class="section-number-3">4.16</span> <span class="done READ">READ</span> Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding</h3>
<div class="outline-text-3" id="text-hundman2018detecting">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-11 Sat 23:22]</span></span></p>
<pre class="example">
CUSTOM_ID: hundman2018detecting
YEAR: 2018
AUTHOR: Hundman, Kyle and Constantinou, Valentino and Laporte, Christopher and Colwell, Ian and Soderstrom, Tom
</pre>

<p>
I picked this paper mostly randomly while looking around for the general scene
of anomaly detection. The general framework looks like this:
</p>

<ol class="org-ol">
<li>Train a model on the sequence</li>
<li>Predict the future while collecting errors</li>
<li>Identify anomalies using a non-parametric <i>heuristic</i></li>
<li>Post-hoc pruning of false positives based on <i>identified</i> anomalies</li>
</ol>

<p>
Even though their method has less knobs to worry about, it still does not <i>feel</i>
auto-pilotish while reading the document. Well, that is supposed to happen I
guess.
</p>
</div>
</div>

<div id="outline-container-org9ac16c9" class="outline-3">
<h3 id="ratner2017snorkel"><span class="section-number-3">4.17</span> <span class="done READ">READ</span> Snorkel: Rapid training data creation with weak supervision</h3>
<div class="outline-text-3" id="text-ratner2017snorkel">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-06 Mon 23:49]</span></span></p>
<pre class="example">
CUSTOM_ID: ratner2017snorkel
YEAR: 2017
AUTHOR: Ratner, Alexander and Bach, Stephen H and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher
</pre>

<p>
This one has bunch of practical upgrades on the original <i>data programming</i> paper.
Two major things here involve:
</p>

<ol class="org-ol">
<li>deciding when to use the generative model (as compared to voting)</li>
<li>tackling correlation</li>
</ol>

<p>
On a side note, while looking at the results you might find that even majority
voting (which is <i>very easy</i> to implement) might not be that bad if you are a
little careful.
</p>
</div>
</div>

<div id="outline-container-orgfa741df" class="outline-3">
<h3 id="sculley2015hidden"><span class="section-number-3">4.18</span> <span class="done READ">READ</span> Hidden technical debt in machine learning systems</h3>
<div class="outline-text-3" id="text-sculley2015hidden">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-25 Thu 00:21]</span></span></p>
<pre class="example">
CUSTOM_ID: sculley2015hidden
YEAR: 2015
AUTHOR: Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan
</pre>

<blockquote>
<p>
It is important to create team cultures that reward deletion of features,
reduction of complexity, improvements in reproducibility, stability, and
monitoring to the same degree that improvements in accuracy are valued.
</p>
</blockquote>

<blockquote>
<p>
Paying down ML-related technical debt requires a specific commitment, which can
often only be achieved by a shift in team culture. Recognizing, prioritizing,
and rewarding this effort is important for the long term health of successful ML
teams.
</p>
</blockquote>

<p>
There are other things too, but I specifically like the clippings above since
they are about things that are very likely to be missed.
</p>
</div>
</div>

<div id="outline-container-org42c50c9" class="outline-3">
<h3 id="bottou2008tradeoffs"><span class="section-number-3">4.19</span> The tradeoffs of large scale learning</h3>
<div class="outline-text-3" id="text-bottou2008tradeoffs">
<pre class="example">
CUSTOM_ID: bottou2008tradeoffs
YEAR: 2008
AUTHOR: Bottou, L{\'e}on and Bousquet, Olivier
</pre>
</div>
</div>

<div id="outline-container-orga083cbd" class="outline-3">
<h3 id="ratner2016data"><span class="section-number-3">4.20</span> <span class="done READ">READ</span> Data programming: Creating large training sets, quickly</h3>
<div class="outline-text-3" id="text-ratner2016data">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-05-06 Mon 23:55]</span></span></p>
<pre class="example">
CUSTOM_ID: ratner2016data
YEAR: 2016
AUTHOR: Ratner, Alexander J and De Sa, Christopher M and Wu, Sen and Selsam, Daniel and R{\'e}, Christopher
</pre>

<p>
Main idea is to focus on creating \(O(1)\) labelling functions on boundless data
to get similar asymptotes as compared to labeled data setting.
</p>

<p>
This same change of focus has another side effect which I agree with:
</p>
<blockquote>
<p>
One of our hopes is that a user without expertise in ML will be more productive
iterating on labeling functions than on features.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org1be9fe3" class="outline-3">
<h3 id="jaeger2004harnessing"><span class="section-number-3">4.21</span> <span class="done READ">READ</span> Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</h3>
<div class="outline-text-3" id="text-jaeger2004harnessing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-22 Mon 23:48]</span></span></p>
<pre class="example">
CUSTOM_ID: jaeger2004harnessing
YEAR: 2004
AUTHOR: Jaeger, Herbert and Haas, Harald
</pre>

<p>
This is the Echo State Network paper (probably not the original one but
sufficiently close). I found it to be a <i>little</i> different than what I had earlier
thought about there being separate inputs and outputs.
</p>
</div>
</div>

<div id="outline-container-orgf8a24a8" class="outline-3">
<h3 id="elsayed2018adversarial"><span class="section-number-3">4.22</span> <span class="done READ">READ</span> Adversarial examples that fool both computer vision and time-limited humans</h3>
<div class="outline-text-3" id="text-elsayed2018adversarial">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-07-18 Sat 00:00]</span></span></p>
<pre class="example">
CUSTOM_ID: elsayed2018adversarial
YEAR: 2018
AUTHOR: Elsayed, Gamaleldin and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alexey and Goodfellow, Ian and Sohl-Dickstein, Jascha
</pre>

<p>
This is an interesting one. Specially since it utilizes the multiple passes
thing that happens in human perception.
</p>
</div>
</div>

<div id="outline-container-org4277303" class="outline-3">
<h3 id="gebru2018datasheets"><span class="section-number-3">4.23</span> <span class="done READ">READ</span> Datasheets for datasets</h3>
<div class="outline-text-3" id="text-gebru2018datasheets">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-05-19 Tue 23:57]</span></span></p>
<pre class="example">
CUSTOM_ID: gebru2018datasheets
YEAR: 2018
AUTHOR: Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daume{\'e} III, Hal and Crawford, Kate
</pre>

<p>
I had read this earlier too if I recall, but this time we ended up incorporating
the idea in our workplace. This has been helpful since the new datasheet
approach makes it really easy to disentangle problems so that people can work on
them very cleanly.
</p>
</div>
</div>

<div id="outline-container-org0091627" class="outline-3">
<h3 id="wan2020nbdt"><span class="section-number-3">4.24</span> <span class="done READ">READ</span> NBDT: Neural-Backed Decision Trees</h3>
<div class="outline-text-3" id="text-wan2020nbdt">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-05-11 Mon 23:56]</span></span></p>
<pre class="example">
CUSTOM_ID: wan2020nbdt
YEAR: 2020
AUTHOR: Wan, Alvin and Dunlap, Lisa and Ho, Daniel and Yin, Jihan and Lee, Scott and Jin, Henry and Petryk, Suzanne and Bargal, Sarah Adel and Gonzalez, Joseph E
</pre>

<p>
This is a nifty trick where we change loss of a regular network to mimic a
decision tree type interpretation of output. Source <a href="https://github.com/alvinwan/neural-backed-decision-trees">here</a>.
</p>
</div>
</div>

<div id="outline-container-orgaf9a2d6" class="outline-3">
<h3 id="bakshy2014designing"><span class="section-number-3">4.25</span> <span class="done READ">READ</span> Designing and deploying online field experiments</h3>
<div class="outline-text-3" id="text-bakshy2014designing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-05-08 Fri 23:55]</span></span></p>
<pre class="example">
CUSTOM_ID: bakshy2014designing
YEAR: 2014
AUTHOR: Bakshy, Eytan and Eckles, Dean and Bernstein, Michael S
</pre>

<p>
Read this to know more about how <a href="https://github.com/facebook/planout">planout</a> is used in real world systems.
</p>
</div>
</div>

<div id="outline-container-org5960335" class="outline-3">
<h3 id="halevy2009unreasonable"><span class="section-number-3">4.26</span> <span class="done READ">READ</span> The unreasonable effectiveness of data</h3>
<div class="outline-text-3" id="text-halevy2009unreasonable">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-03-01 Sun 13:09]</span></span></p>
<pre class="example">
CUSTOM_ID: halevy2009unreasonable
YEAR: 2009
AUTHOR: Halevy, Alon and Norvig, Peter and Pereira, Fernando
</pre>
</div>
</div>

<div id="outline-container-org0011b7f" class="outline-3">
<h3 id="chang2016credit"><span class="section-number-3">4.27</span> <span class="done READ">READ</span> A credit assignment compiler for joint prediction</h3>
<div class="outline-text-3" id="text-chang2016credit">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-02-05 Wed 12:51]</span></span></p>
<pre class="example">
CUSTOM_ID: chang2016credit
YEAR: 2016
AUTHOR: Chang, Kai-Wei and He, He and Ross, Stephane and Daume III, Hal and Langford, John
</pre>

<p>
This talks about an API for framing L2S style search problems in style of an
imperative program which allows for two optimizations:
</p>

<ol class="org-ol">
<li>memoization</li>
<li>forced path collapse, getting losses without going to the last state</li>
</ol>

<p>
Main reduction that happens here is to a cost-sensitive classification problem.
</p>
</div>
</div>

<div id="outline-container-org8b1db0d" class="outline-3">
<h3 id="freund1997decision"><span class="section-number-3">4.28</span> A decision-theoretic generalization of on-line learning and an application to boosting</h3>
<div class="outline-text-3" id="text-freund1997decision">
<pre class="example">
Custom_ID: freund1997decision
AUTHOR: Freund \& Schapire
JOURNAL: Journal of computer and system sciences
YEAR: 1997
VOLUME: 55
PAGES: 119--139
</pre>
</div>
</div>
</div>

<div id="outline-container-org72d0077" class="outline-2">
<h2 id="sec-computing/programming"><span class="section-number-2">5</span> Computing/Programming</h2>
<div class="outline-text-2" id="text-sec-computing/programming">
<pre class="example">
</pre>
</div>

<div id="outline-container-org49dec92" class="outline-3">
<h3 id="appuswamy2013nobody"><span class="section-number-3">5.1</span> <span class="done READ">READ</span> Nobody ever got fired for buying a cluster</h3>
<div class="outline-text-3" id="text-appuswamy2013nobody">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-06-07 Sun 23:58]</span></span></p>
<pre class="example">
CUSTOM_ID: appuswamy2013nobody
YEAR: 2013
AUTHOR: Appuswamy, Raja and Gkantsidis, Christos and Narayanan, Dushyanth and Hodson, Orion and Rowstron, Ant
</pre>
</div>
</div>

<div id="outline-container-org23081ac" class="outline-3">
<h3 id="kiczales1993metaobject"><span class="section-number-3">5.2</span> <span class="done READ">READ</span> Metaobject protocols: Why we want them and what else they can do</h3>
<div class="outline-text-3" id="text-kiczales1993metaobject">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-10-28 Mon 23:08]</span></span></p>
<pre class="example">
CUSTOM_ID: kiczales1993metaobject
YEAR: 1993
AUTHOR: Kiczales, Gregor and Ashley, J Michael and Rodriguez, Luis and Vahdat, Amin and Bobrow, Daniel G
</pre>

<p>
This provides a wider perspective on MOP. Specially the sections on Scheme
extension techniques clarified that MOP is a very general way of creating an
extension system for something else.
</p>
</div>
</div>

<div id="outline-container-org03a123d" class="outline-3">
<h3 id="thompson1984reflections"><span class="section-number-3">5.3</span> <span class="done READ">READ</span> Reflections on trusting trust</h3>
<div class="outline-text-3" id="text-thompson1984reflections">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-08-20 Tue 00:42]</span></span></p>
<pre class="example">
CUSTOM_ID: thompson1984reflections
YEAR: 1984
AUTHOR: Thompson, Ken and others
</pre>

<p>
I remember reading this or watching the talk 3-4 years earlier but not
understanding what Ken was trying to say. This time it was fine. I like this top
blurb:
</p>

<blockquote>
<p>
To what extent should one trust a statement that a program is free of Trojan
horses? Perhaps it is more important to trust the people who wrote the software.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgf6b05ae" class="outline-3">
<h3 id="hellerstein1997online"><span class="section-number-3">5.4</span> <span class="done READ">READ</span> Online aggregation</h3>
<div class="outline-text-3" id="text-hellerstein1997online">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-17 Wed 02:12]</span></span></p>
<pre class="example">
CUSTOM_ID: hellerstein1997online
YEAR: 1997
AUTHOR: Hellerstein, Joseph M and Haas, Peter J and Wang, Helen J
</pre>

<p>
I was looking into this while looking for prior works that provide streaming
results from a database. The idea is to have always available results for
aggregation queries like <code>SUM</code>, <code>COUNT</code> etc. along with uncertainty measurements
based on the currently sampled tuples. Other than the uncertainty estimation
formulations, they presented work on the implementation side of the idea which
involves random sampling and various UX niceties.
</p>
</div>
</div>

<div id="outline-container-orgf5c77ad" class="outline-3">
<h3 id="lindahl2006practical"><span class="section-number-3">5.5</span> <span class="done READ">READ</span> Practical type inference based on success typings</h3>
<div class="outline-text-3" id="text-lindahl2006practical">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-31 Sun 20:56]</span></span></p>
<pre class="example">
CUSTOM_ID: lindahl2006practical
YEAR: 2006
AUTHOR: Lindahl, Tobias and Sagonas, Konstantinos
</pre>

<p>
The general idea is to allow all programs that throw no runtime errors. This is
specially useful in languages which are philosophically dynamic. I like this
approach towards types since programming in a dynamic language involves dropping
a lot of so called writer's 'intention' here and there which does not adhere to
the static type philosophy.
</p>

<p>
Not sure if this is one of the firsts (the first for functional languages
according to the paper), but these days there are many mainstream dynamic
languages adopting such soft typing systems in some form.
</p>
</div>
</div>

<div id="outline-container-orgaa8de61" class="outline-3">
<h3 id="tratt2009dynamically"><span class="section-number-3">5.6</span> <span class="done READ">READ</span> Dynamically typed languages</h3>
<div class="outline-text-3" id="text-tratt2009dynamically">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-19 Tue 00:10]</span></span></p>
<pre class="example">
CUSTOM_ID: tratt2009dynamically
YEAR: 2009
AUTHOR: Tratt, Laurence
</pre>

<p>
A basic and exhaustive intro to dynamic typed languages. Good for beginners.
</p>
</div>
</div>

<div id="outline-container-org46e3afd" class="outline-3">
<h3 id="steele1999growing"><span class="section-number-3">5.7</span> <span class="done READ">READ</span> Growing a language</h3>
<div class="outline-text-3" id="text-steele1999growing">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-03-12 Tue 11:11]</span></span></p>
<pre class="example">
CUSTOM_ID: steele1999growing
YEAR: 1999
AUTHOR: Steele, Guy L
</pre>

<p>
This is originally a talk, I read <a href="https://www.cs.virginia.edu/~evans/cs655/readings/steele.pdf">a pdf</a> version. An interesting thing is the way
the talk itself is structured (its vocabulary mostly) exemplifying the same
<i>growth</i> mechanism that Guy talks about in relation to languages.
</p>
</div>
</div>

<div id="outline-container-orgc6b7895" class="outline-3">
<h3 id="agarwal2013blinkdb"><span class="section-number-3">5.8</span> <span class="done READ">READ</span> BlinkDB: queries with bounded errors and bounded response times on very large data</h3>
<div class="outline-text-3" id="text-agarwal2013blinkdb">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-04-07 Sun 21:57]</span></span></p>
<pre class="example">
CUSTOM_ID: agarwal2013blinkdb
YEAR: 2013
AUTHOR: Agarwal, Sameer and Mozafari, Barzan and Panda, Aurojit and Milner, Henry and Madden, Samuel and Stoica, Ion
</pre>

<p>
I find works like this, and other approximate query processing systems, pretty
interesting since their general structures are close to machine learning systems
with slightly different metrics to be optimized. This, of course, then provides
a lot of food for thought.
</p>

<p>
So, what BlinkDB does is pretty clear from the title. On the how side, they
basically create a bunch of samples (table subsets) based on criterion derived
from past queries. The 'key' for samples here are sets of columns involved in
the queries' <code>WHERE</code>, <code>HAVING</code> etc. clauses. When asked a query with timing, error
requirements, a sample is picked (after some estimation on some data; this is
important, since they don't want to put much assumptions on the type of
workload) and query runs on that.
</p>

<p>
Since they are mostly for high scale use cases, these methods are not 'very'
visible unless you are into such things. Although, I believe, similar ideas (I
specially liked the Online Aggregation thing from 1997) can be put in more
commonplace, smaller, systems (or already are there).
</p>
</div>
</div>

<div id="outline-container-org756afb9" class="outline-3">
<h3 id="chang2017type"><span class="section-number-3">5.9</span> Type systems as macros</h3>
<div class="outline-text-3" id="text-chang2017type">
<pre class="example">
CUSTOM_ID: chang2017type
YEAR: 2017
AUTHOR: Chang, Stephen and Knauth, Alex and Greenman, Ben
</pre>
</div>
</div>

<div id="outline-container-org0022743" class="outline-3">
<h3 id="baez2010physics"><span class="section-number-3">5.10</span> Physics, topology, logic and computation: a Rosetta Stone</h3>
<div class="outline-text-3" id="text-baez2010physics">
<pre class="example">
Custom_ID: baez2010physics
AUTHOR: Baez \& Stay
YEAR: 2010
</pre>
</div>
</div>

<div id="outline-container-org9097c6e" class="outline-3">
<h3 id="o2009genuine"><span class="section-number-3">5.11</span> <span class="done READ">READ</span> The Genuine Sieve of Eratosthenes</h3>
<div class="outline-text-3" id="text-o2009genuine">
<pre class="example">
Custom_ID: o2009genuine
AUTHOR: O'NEILL
JOURNAL: Journal of Functional Programming
YEAR: 2009
VOLUME: 19
PAGES: 95--106
</pre>

<p>
This talks about a functional implementation of Sieve of Eratosthenes.
Specifically it debunks the following incorrect implementation:
</p>

<div class="org-src-container">
<pre class="src src-haskell"><span class="org-haskell-definition">primes</span> <span class="org-haskell-operator">=</span> sieve <span class="org-rainbow-delimiters-depth-1">[</span><span class="org-highlight-numbers-number">2</span><span class="org-haskell-operator">..</span><span class="org-rainbow-delimiters-depth-1">]</span>
<span class="org-haskell-definition">sieve</span> <span class="org-rainbow-delimiters-depth-1">(</span>p <span class="org-haskell-constructor">:</span> xs<span class="org-rainbow-delimiters-depth-1">)</span> <span class="org-haskell-operator">=</span> p <span class="org-haskell-constructor">:</span> sieve <span class="org-rainbow-delimiters-depth-1">[</span>x <span class="org-haskell-operator">|</span> x <span class="org-haskell-operator">&lt;&#8722;</span> xs, x <span class="org-haskell-operator">`mod`</span> p <span class="org-haskell-operator">&gt;</span> <span class="org-highlight-numbers-number">0</span><span class="org-rainbow-delimiters-depth-1">]</span>
</pre>
</div>

<p>
Then we see correct functional implementations with neat tricks made possible
due to laziness of Haskell. Although slower, there is a list based
implementation by Bird mentioned in the <i>Epilogue</i> which is pretty readable (and
elegant) and follows very closely the following description:
</p>

<pre class="example">
primes = [2, 3, ...] \ [[p², p²+p, ...] for p in primes]
</pre>
</div>
</div>

<div id="outline-container-orgbc42225" class="outline-3">
<h3 id="hughes1989functional"><span class="section-number-3">5.12</span> <span class="done READ">READ</span> Why functional programming matters</h3>
<div class="outline-text-3" id="text-hughes1989functional">
<pre class="example">
Custom_ID: hughes1989functional
AUTHOR: Hughes
JOURNAL: The computer journal
YEAR: 1989
VOLUME: 32
PAGES: 98--107
</pre>

<p>
This is a famous paper and I wanted to see what it focuses on. It's basically
about the following two properties and their effect on modularity in functional
programmings:
</p>

<ol class="org-ol">
<li>Higher order functions</li>
<li>Lazy evaluation</li>
</ol>

<p>
The examples are nice and make this is a good read for beginners. Though I
suspect there might be better, recent, articles on these topics now.
</p>
</div>
</div>
</div>

<div id="outline-container-org33ba46e" class="outline-2">
<h2 id="sec-misc"><span class="section-number-2">6</span> Misc</h2>
<div class="outline-text-2" id="text-sec-misc">
<pre class="example">
</pre>
</div>
<div id="outline-container-org28f20d9" class="outline-3">
<h3 id="chapman1988research"><span class="section-number-3">6.1</span> <span class="done READ">READ</span> How to do research at the MIT AI lab</h3>
<div class="outline-text-3" id="text-chapman1988research">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2020-04-11 Sat 22:10]</span></span></p>
<pre class="example">
CUSTOM_ID: chapman1988research
YEAR: 1988
AUTHOR: Chapman, David
</pre>

<p>
A bit dated but has nice nuggets of wisdom. I like simple pointers like "writing
is debugging" which provides a different perspective to the way a few trivial
things are done.
</p>

<p>
Thanks to <a href="https://github.com/jaydeepborkar">Jaydeep</a> for pointing me to this.
</p>
</div>
</div>

<div id="outline-container-org307e065" class="outline-3">
<h3 id="conway1968committees"><span class="section-number-3">6.2</span> <span class="done READ">READ</span> How do committees invent</h3>
<div class="outline-text-3" id="text-conway1968committees">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-12-15 Sun 15:19]</span></span></p>
<pre class="example">
CUSTOM_ID: conway1968committees
YEAR: 1968
AUTHOR: Conway, Melvin E
</pre>

<p>
Original paper on Conway's Law. The reasoning used is easy to understand and
feels trivial in hindsight but there are nice nuggets scattered in between, like
the following, which makes the reading worthwhile:
</p>

<blockquote>
<p>
A manager knows that he will be vulnerable to the charge of mismanagement if he
misses his schedule without having applied all his resources.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org156d0d6" class="outline-3">
<h3 id="anderson1972more"><span class="section-number-3">6.3</span> <span class="done READ">READ</span> More is different</h3>
<div class="outline-text-3" id="text-anderson1972more">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2019-01-27 Sun 20:55]</span></span></p>
<pre class="example">
CUSTOM_ID: anderson1972more
YEAR: 1972
AUTHOR: Anderson, Philip W and others
</pre>

<p>
The basic idea is the following:
</p>

<blockquote>
<p>
The main fallacy in this kind of thinking is that reductionist hypothesis does
not by any means imply a "constructionist" one.
</p>
</blockquote>

<p>
We are trying to understand that reductionist view is not going to explain
everything and that fundamental laws at the lowest level are not going to be <i>the
fundamental</i> ones for the higher level ("Psychology is not applied biology&#x2026;").
A littl0e hierarchy is also presented using examples where our movements across
levels results in <i>broken symmetry</i>:
</p>

<ul class="org-ul">
<li>Crystallinity</li>
<li>Functional structures</li>
<li><i>Regular systems</i> with information like DNA</li>
<li>Ordering in the time dimension for information processing etc.</li>
</ul>

<blockquote>
<p>
So it is not true, as a recent article would have it, that we each should
"cultivate out own valley, and not attempt to build roads over the mountain
ranges &#x2026; between the sciences." Rather, we should recognize that such roads,
while often the quickest shortcut to another part of our own science, are not
visible from the viewpoint of one science alone.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-orgc16cd9c" class="outline-3">
<h3 id="spector2012google"><span class="section-number-3">6.4</span> <span class="done READ">READ</span> Google's hybrid approach to research</h3>
<div class="outline-text-3" id="text-spector2012google">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-30 Tue 02:34]</span></span></p>
<pre class="example">
CUSTOM_ID: spector2012google
YEAR: 2012
AUTHOR: Spector, Alfred and Norvig, Peter and Petrov, Slav
</pre>

<p>
Mostly about the people being <i>researchers</i> and <i>developers</i> and how it affects
various aspects of experiments.
</p>
</div>
</div>

<div id="outline-container-org2191b20" class="outline-3">
<h3 id="sculley2014machine"><span class="section-number-3">6.5</span> <span class="done READ">READ</span> Machine learning: The high-interest credit card of technical debt</h3>
<div class="outline-text-3" id="text-sculley2014machine">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-20 Sat 02:33]</span></span></p>
<pre class="example">
CUSTOM_ID: sculley2014machine
YEAR: 2014
AUTHOR: Sculley, D and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael
</pre>
</div>
</div>

<div id="outline-container-org6a04d55" class="outline-3">
<h3 id="gabriel2010better"><span class="section-number-3">6.6</span> <span class="done READ">READ</span> Better science through art</h3>
<div class="outline-text-3" id="text-gabriel2010better">
<p><span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED:</span> <span class="timestamp">[2018-10-12 Fri 23:16] </span> <span class="timestamp-kwd">SCHEDULED:</span> <span class="timestamp">&lt;2018-10-06 Sat&gt;</span></span></p>
<pre class="example">
CUSTOM_ID: gabriel2010better
YEAR: 2010
AUTHOR: Gabriel, Richard P and Sullivan, Kevin J
</pre>

<p>
Here are the last few lines which cover what's common between Science and Art
and also summarize the document:
</p>

<ul class="org-ul">
<li>Explore: wander / defamiliarize</li>
<li>Discover: guess / abduce</li>
<li>Understand: validate / ask—did you build the right thing?</li>
</ul>
</div>
</div>

<div id="outline-container-org30dba6d" class="outline-3">
<h3 id="verna2018lisp"><span class="section-number-3">6.7</span> <span class="done READ">READ</span> Lisp, Jazz, Aikido&#x2013;Three Expressions of a Single Essence</h3>
<div class="outline-text-3" id="text-verna2018lisp">
<pre class="example">
Custom_ID: verna2018lisp
AUTHOR: Verna
JOURNAL: arXiv preprint arXiv:1804.00485
YEAR: 2018
</pre>

<p>
Okay, this was up on /r/lisp, felt not that much effort to read so I gave it a
shot. There are three general aesthetic avenues that the author covers:
</p>

<ol class="org-ol">
<li>Conformation</li>
<li>Transgression</li>
<li>Unification</li>
</ol>

<p>
The general idea is about the similar interplay of these in all the 3 things
(Lisp, Jazz &amp; Aikido) and how they end up being a source of pleasure and
enlightenment.
</p>

<p>
From whatever I have felt, <i>things</i> that focus on an act itself (rather than
prioritizing the results) end up being like these (well, probably this is
obvious).
</p>

<p>
This paper is a quick read and is not overly philosophical. Maybe that's because
one of the focus is on tools that stay out of your way by staying <i>practical</i> (you
can see this when the author talks about <i>Common Lisp</i> specifically). Although I
must say that I know next to nothing about both <i>Jazz</i> and <i>Aikido</i> so might not
have really been able to connect all the pieces.
</p>


<p>
<h1 class='org-ref-bib-h1'>Bibliography</h1>
<ul class='org-ref-bib'><li><a id="bak1988self">[bak1988self]</a> <a name="bak1988self"></a>Bak, Tang & Wiesenfeld. 1988. "Self-organized criticality." <i>Physical review A</i>, 38(1), 364. <a class="bib-link" href="">link</a>. <a class="bib-link" href="http://dx.doi.org/">doi</a>.</li>
<li><a id="packard1988adaptation">[packard1988adaptation]</a> <a name="packard1988adaptation"></a>Packard. 1988. "Adaptation toward the edge of chaos." <i>Dynamic patterns in complex systems</i>, 212, 293-301. <a class="bib-link" href="">link</a>. <a class="bib-link" href="http://dx.doi.org/">doi</a>.</li>
<li><a id="broido2018scale">[broido2018scale]</a> <a name="broido2018scale"></a>Broido & Clauset. 2018. "Scale-free networks are rare." <i>arXiv preprint arXiv:1801.03400</i>, , <a class="bib-link" href="">link</a>. <a class="bib-link" href="http://dx.doi.org/">doi</a>.</li>
<li><a id="papangelis2019collaborative">[papangelis2019collaborative]</a> <a name="papangelis2019collaborative"></a>@miscpapangelis2019collaborative,
  Author = Papangelis, Alexandros and Wang, Yi-Chia and Molino, Piero and Tur, Gokhan,
  Title = Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning,
  Year = 2019,
  Eprint = arXiv:1907.05507,
</li>
<li><a id="sennrich2015neural">[sennrich2015neural]</a> <a name="sennrich2015neural"></a>Sennrich, Haddow & Birch. 2015. "Neural machine translation of rare words with subword units." <i>arXiv preprint arXiv:1508.07909</i>, , <a class="bib-link" href="">link</a>. <a class="bib-link" href="http://dx.doi.org/">doi</a>.</li>
<li><a id="gage1994new">[gage1994new]</a> <a name="gage1994new"></a>Gage. 1994. "A new algorithm for data compression." <i>The C Users Journal</i>, 12(2), 23-38. <a class="bib-link" href="">link</a>. <a class="bib-link" href="http://dx.doi.org/">doi</a>.</li>
<li><a id="ratner2017snorkel">[ratner2017snorkel]</a> <a name="ratner2017snorkel"></a>Ratner, Bach, Ehrenberg, Fries, Wu & R\'e. 2017. "Snorkel: Rapid training data creation with weak supervision." <i>Proceedings of the VLDB Endowment</i>, 11(3), 269-282. <a class="bib-link" href="">link</a>. <a class="bib-link" href="http://dx.doi.org/">doi</a>.</li>
</ul>
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">Might have missed somewhere or there might be mentions in
prior works</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<footer id='footer'></footer>
</div>
</body>
</html>
